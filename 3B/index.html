<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>CS180/280A Projects 3A & 3B — Rectification, Mosaics, and Autostitching</title>
  <meta name="description" content="Combined CS180 project 3A + 3B webpage: Homography, inverse warping, mosaics (3A) and Harris corners, descriptors, matching, RANSAC mosaics (3B)." />
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;600;800&display=swap" rel="stylesheet">
  <style>
    :root {
      --bg: #0b0e12;
      --ink: #e9edf2;
      --ink-dim: #b8c2cf;
      --card: #121821;
      --accent: #6aa1ff;
      --muted: #2a3340;
    }
    html, body { margin: 0; padding: 0; font-family: Inter, system-ui, -apple-system, Segoe UI, Roboto, "Helvetica Neue", Arial, "Noto Sans", "Apple Color Emoji", "Segoe UI Emoji"; background: var(--bg); color: var(--ink); }
    a { color: var(--accent); text-decoration: none; }
    a:hover { text-decoration: underline; }
    .container { max-width: 1100px; margin: 0 auto; padding: 2rem 1.25rem 4rem; }
    header { padding: 2.5rem 0 1.25rem; }
    .title { font-size: clamp(1.8rem, 2.4vw, 2.6rem); font-weight: 800; letter-spacing: 0.2px; margin: 0 0 0.25rem; }
    .subtitle { margin: 0.25rem 0 0; color: var(--ink-dim); }
    nav { position: sticky; top: 0; background: linear-gradient(180deg, rgba(11,14,18,0.95), rgba(11,14,18,0.7)); backdrop-filter: blur(8px); border-bottom: 1px solid var(--muted); z-index: 20; }
    nav .nav-inner { max-width: 1100px; margin: 0 auto; padding: 0.5rem 1.25rem; display: flex; gap: 1rem; flex-wrap: wrap; }
    nav a { font-size: 0.95rem; padding: 0.4rem 0.6rem; border-radius: 0.5rem; }
    nav a:hover { background: var(--muted); text-decoration: none; }

    section { padding: 2.25rem 0; border-bottom: 1px solid var(--muted); }
    h2 { font-size: clamp(1.4rem, 2vw, 1.8rem); margin: 0 0 0.75rem; }
    h3 { font-size: 1.05rem; margin: 1.25rem 0 0.25rem; color: var(--ink-dim); font-weight: 600; text-transform: uppercase; letter-spacing: 0.08em; }
    p { line-height: 1.7; color: var(--ink-dim); }
    ol, ul { color: var(--ink-dim); line-height: 1.7; }
    .callout { background: #0f141c; border: 1px solid var(--muted); border-radius: 0.9rem; padding: 1rem 1rem; }

    .grid { display: grid; gap: 0.9rem; }
    .grid.cols-2 { grid-template-columns: repeat(2, minmax(0, 1fr)); }
    .grid.cols-3 { grid-template-columns: repeat(3, minmax(0, 1fr)); }
    .grid.cols-4 { grid-template-columns: repeat(4, minmax(0, 1fr)); }
    @media (max-width: 880px) { .grid.cols-4 { grid-template-columns: repeat(2, 1fr); } }
    @media (max-width: 680px) { .grid.cols-3, .grid.cols-2 { grid-template-columns: 1fr; } }

    /* Wide-only images (horizontal visuals) */
    .grid figure.wide { grid-column: 1 / -1; }
    .grid figure.wide img { display: block; width: 100%; height: auto; }

    figure { margin: 0; background: var(--card); border: 1px solid var(--muted); border-radius: 1rem; overflow: hidden; }
    figure img { display: block; width: 100%; height: auto; }
    figcaption { padding: 0.75rem 0.9rem; font-size: 0.95rem; color: var(--ink-dim); border-top: 1px solid var(--muted); }

    .code { font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace; background: #0f141c; border: 1px solid var(--muted); border-radius: 0.6rem; padding: 0.75rem 0.9rem; color: var(--ink); overflow-x: auto; }

    .footer { padding-top: 2rem; color: var(--ink-dim); font-size: 0.95rem; }
    .tag { display: inline-block; background: #0f141c; border: 1px solid var(--muted); color: var(--ink-dim); padding: 0.15rem 0.5rem; border-radius: 0.5rem; font-size: 0.85rem; margin-right: 0.35rem; }
  </style>
</head>
<body>
  <nav>
    <div class="nav-inner">
      <a href="#overview">Overview</a>
      <a href="#a1">A.1 Raw Inputs</a>
      <a href="#a2">A.2 Homography</a>
      <a href="#a3">A.3 Rectification</a>
      <a href="#a4">A.4 Mosaics</a>
      <a href="#b1">B.1 Harris Corners</a>
      <a href="#b2">B.2 Descriptors</a>
      <a href="#b3">B.3 Matching</a>
      <a href="#b4">B.4 RANSAC & Mosaics</a>
      <a href="#results">Results Gallery</a>
      <a href="#implementation">Implementation Notes</a>
      <a href="#references">References</a>
    </div>
  </nav>

  <div class="container">
    <header>
      <h1 class="title">Projects 3A & 3B — Planar Rectification, Mosaics & Automatic Stitching</h1>
      <p class="subtitle">CS180/280A – Intro to Computer Vision & Computational Photography</p>
      <p class="subtitle">Eduardo Cortes · Fall 2025</p>
    </header>

    <section id="overview">
      <h2>Overview</h2>
      <p>
        This combined page covers <strong>3A</strong> (manual correspondences → homography → inverse warping → mosaics) and <strong>3B</strong> (automatic corner detection, patch descriptors, feature matching, RANSAC homography, and mosaics).
      </p>
      <p class="callout"><strong>What problem are we solving?</strong> Given overlapping photos of the same scene, align them onto a shared coordinate system and blend them into a single panorama. 3A solves the geometry with human-picked correspondences; 3B automates the correspondences and makes the pipeline end-to-end.</p>
      <ol>
        <li><em>3A:</em> pick point pairs, solve a <strong>homography</strong>, <strong>inverse-warp</strong> images, and <strong>feather-blend</strong>.</li>
        <li><em>3B:</em> detect <strong>corners</strong>, extract <strong>descriptors</strong>, <strong>match</strong> with a ratio test, run <strong>RANSAC</strong>, warp & blend.</li>
      </ol>
    </section>

    <!-- ===================== 3A CONTENT (with walkthroughs) ===================== -->
    <section id="a1">
      <h2>A.1 – Raw Inputs (Project 3A)</h2>
      <p>
        The 3A portion uses overlapping photos (three per scene) for mosaics and two oblique shots for rectification examples. Below, we also include the I/O helpers used across 3A so you can see the exact pixel plumbing.
      </p>

      <h3>Implementation — EXIF-aware I/O helpers</h3>
      <pre class="code">def read_image(path: str) -> np.ndarray:
    image = Image.open(path)
    image = ImageOps.exif_transpose(image)
    array = np.asarray(image).astype(np.float32) / 255.0
    if array.ndim == 2:
        array = np.stack([array, array, array], axis=-1)
    return array

def save_image(path: str, array: np.ndarray):
    os.makedirs(os.path.dirname(path), exist_ok=True)
    array8 = (np.clip(array, 0, 1) * 255).astype(np.uint8)
    Image.fromarray(array8).save(path, exif=b"")

def to_grayscale(rgb: np.ndarray) -> np.ndarray:
    if rgb.ndim == 2:
        return rgb.astype(np.float32)
    r, g, b = rgb[..., 0], rgb[..., 1], rgb[..., 2]
    return (0.2989 * r + 0.5870 * g + 0.1140 * b).astype(np.float32)</pre>

      <p class="callout"><strong>Walkthrough.</strong> <em>read_image</em> loads and applies EXIF orientation so pixels match on-screen orientation; returns float32 in [0,1]. If grayscale, it replicates to 3 channels so later code expecting RGB doesn’t break. <em>save_image</em> clips, converts to uint8, and strips EXIF to avoid double rotations. <em>to_grayscale</em> uses standard luminance weights.</p>

      <div class="grid cols-3">
        <figure><img src="images_A/a1_set1_src01.jpg" alt="Set 1 – src01"><figcaption>Set 1 · src01</figcaption></figure>
        <figure><img src="images_A/a1_set1_src02.jpg" alt="Set 1 – src02"><figcaption>Set 1 · src02</figcaption></figure>
        <figure><img src="images_A/a1_set1_src03.jpg" alt="Set 1 – src03"><figcaption>Set 1 · src03</figcaption></figure>
        <figure><img src="images_A/a1_set2_src01.jpg" alt="Set 2 – src01"><figcaption>Set 2 · src01</figcaption></figure>
        <figure><img src="images_A/a1_set2_src02.jpg" alt="Set 2 – src02"><figcaption>Set 2 · src02</figcaption></figure>
        <figure><img src="images_A/a1_set2_src03.jpg" alt="Set 2 – src03"><figcaption>Set 2 · src03</figcaption></figure>
        <figure><img src="images_A/a1_set3_src01.jpg" alt="Set 3 – src01"><figcaption>Set 3 · src01</figcaption></figure>
        <figure><img src="images_A/a1_set3_src02.jpg" alt="Set 3 – src02"><figcaption>Set 3 · src02</figcaption></figure>
        <figure><img src="images_A/a1_set3_src03.jpg" alt="Set 3 – src03"><figcaption>Set 3 · src03</figcaption></figure>
      </div>

      <h3>Rectification sources</h3>
      <div class="grid cols-2">
        <figure><img src="images_A/a1_rect_poster_raw.jpg" alt="Rectification – poster"><figcaption>Poster (oblique)</figcaption></figure>
        <figure><img src="images_A/a1_rect_tiles_raw.jpg" alt="Rectification – tiles"><figcaption>Tiles (oblique)</figcaption></figure>
      </div>
    </section>

    <section id="a2">
      <h2>A.2 – Homography (Manual Correspondences → 3×3 H)</h2>
      <p>
        A <strong>homography</strong> is a 3×3 matrix that maps any point on a plane from one image to another. In 3A, we select corresponding points by hand and then solve for <em>H</em> using a numerically stable, <em>normalized DLT</em>.
      </p>

      <h3>Implementation — Normalized DLT</h3>
      <pre class="code">def _normalize_points(xy: np.ndarray):
    mu = xy.mean(axis=0)
    d  = np.sqrt(((xy - mu) ** 2).sum(axis=1)).mean()
    s  = np.sqrt(2.0) / max(d, 1e-8)
    T  = np.array([[s, 0, -s * mu[0]],
                   [0, s, -s * mu[1]],
                   [0, 0, 1.0]], dtype=np.float64)
    xy_h = np.c_[xy, np.ones(len(xy))]
    xy_n = (T @ xy_h.T).T
    return T, xy_n[:, :2]

def dlt_homography(xy_src: np.ndarray, xy_dst: np.ndarray) -> np.ndarray:
    assert xy_src.shape == xy_dst.shape and xy_src.shape[0] >= 4
    T_src, src_n = _normalize_points(xy_src)
    T_dst, dst_n = _normalize_points(xy_dst)
    A = []
    for (x, y), (u, v) in zip(src_n, dst_n):
        A.append([-x, -y, -1, 0, 0, 0, u * x, u * y, u])
        A.append([ 0,  0,  0,-x,-y,-1, v * x, v * y, v])
    A = np.asarray(A, dtype=np.float64)
    _, _, Vt = np.linalg.svd(A)
    Hn = Vt[-1].reshape(3, 3)
    H  = np.linalg.inv(T_dst) @ Hn @ T_src
    if abs(H[2,2]) > 1e-12: H /= H[2,2]
    return H</pre>

      <h3>Walkthrough (deep dive)</h3>
      <ol>
        <li><strong>Collect reliable pairs.</strong> Pick at least 4 non‑collinear correspondences on the same physical plane (e.g., corners of posters, tile intersections). More points (6–12) help average out click noise; spread them across the plane to avoid degeneracy.</li>
        <li><strong>Normalize both sets.</strong> <code>_normalize_points</code> recenters around the mean and scales so the average radius is √2. This makes the linear system well‑conditioned so SVD finds a stable nullspace vector. Skipping normalization often produces wildly different results depending on the coordinate scale.</li>
        <li><strong>Build the DLT system.</strong> Each pair contributes two equations derived from <code>H [x,y,1]^T ∝ [u,v,1]^T</code>. Stacking all rows yields <code>A h = 0</code> up to scale. We solve for the last singular vector (the direction of minimal error).</li>
        <li><strong>Denormalize & scale.</strong> Map <code>Hn</code> back to pixel space by <code>T_dst^{-1} · Hn · T_src</code>; divide so <code>H[2,2]=1</code> for readability. This doesn’t change the mapping, just the representation.</li>
        <li><strong>Sanity checks.</strong> (a) Reproject the input points and report the RMS pixel error; you should see sub‑pixel to a few pixels depending on click noise. (b) Visualize epipolar‑looking distortions—if straight lines bend, you probably mixed planes or mis‑clicked.</li>
        <li><strong>Edge cases.</strong> Four nearly collinear points make <code>A</code> rank‑deficient → unstable <em>H</em>. Avoid all points being from a tiny patch: lever arm is too short and errors explode away from the patch.</li>
      </ol>

      <h3>Applying H to points (numerical safety)</h3>
      <pre class="code">def apply_homography_points(xy: np.ndarray, H: np.ndarray) -> np.ndarray:
    xy_h = np.c_[xy, np.ones(len(xy))]
    uvw  = (H @ xy_h.T).T
    w = uvw[:, 2:3]
    out = np.full((len(xy), 2), np.nan, dtype=np.float64)
    good = np.abs(w) > 1e-8
    if np.any(good):
        out[good[:, 0]] = uvw[good[:, 0], :2] / w[good[:, 0]]
    return out</pre>
      <p class="callout"><strong>Why check <code>w</code>?</strong> Points that map near the projective horizon (<code>w ≈ 0</code>) would blow up under division. Marking them as <code>NaN</code> prevents downstream crashes and makes debugging footprints easier.</p>
    </section>

    <section id="a3">
      <h2>A.3 – Warping &amp; Rectification</h2>
      <p>
        Given <em>H</em> from the oblique view to a fronto‑parallel target, we render a rectified image using <strong>inverse warping</strong>: for each output pixel, we ask “where in the source would this have come from?” and sample there. Inverse warping avoids holes and duplicated pixels.
      </p>

      <h3>Implementation — Inverse Warping + Bilinear Sampling</h3>
      <pre class="code">def bilinear_sample(img: np.ndarray, x: float, y: float):
    H, W = img.shape[:2]
    if x < 0 or y < 0 or x > W - 1 or y > H - 1:
        return None, False
    x0 = int(np.floor(x)); x1 = min(x0 + 1, W - 1)
    y0 = int(np.floor(y)); y1 = min(y0 + 1, H - 1)
    ax = x - x0; ay = y - y0
    I00 = img[y0, x0]; I10 = img[y0, x1]
    I01 = img[y1, x0]; I11 = img[y1, x1]
    I0 = (1 - ax) * I00 + ax * I10
    I1 = (1 - ay) * I01 + ay * I11
    return (1 - ay) * I0 + ay * I1, True

def inverse_warp_to_canvas(src_img: np.ndarray, H_inv: np.ndarray,
                           out_h: int, out_w: int, offset_xy: tuple[int,int],
                           mode: str = "bilinear"):
    ox, oy = offset_xy
    C = src_img.shape[2] if src_img.ndim == 3 else 1
    out   = np.zeros((out_h, out_w, C), dtype=np.float32)
    alpha = np.zeros((out_h, out_w), dtype=np.float32)
    sampler = bilinear_sample if mode == "bilinear" else sample_nearest
    h0, h1, h2 = H_inv[0], H_inv[1], H_inv[2]
    for Y in range(out_h):
        for X in range(out_w):
            Xc, Yc = float(X), float(Y)
            w = h2[0]*(Xc-ox) + h2[1]*(Yc-oy) + h2[2]
            if abs(w) < 1e-12: continue
            x = (h0[0]*(Xc-ox) + h0[1]*(Yc-oy) + h0[2]) / w
            y = (h1[0]*(Xc-ox) + h1[1]*(Yc-oy) + h1[2]) / w
            val, ok = sampler(src_img, x, y)
            if ok:
                out[Y, X] = val
                alpha[Y, X] = 1.0
    return out, alpha</pre>

      <h3>Walkthrough (pixel‑accurate rectification)</h3>
      <ol>
        <li><strong>Target geometry.</strong> Decide the rectified canvas size by measuring the expected real‑world aspect ratio (e.g., poster width:height) or by warping four corners and taking their bounding box. Bigger canvases preserve detail but cost time.</li>
        <li><strong>Back‑project each output pixel.</strong> We pre‑split <code>H⁻¹</code> rows (<code>h0,h1,h2</code>) so the tight inner loop only does a handful of multiplies and one division.</li>
        <li><strong>Interpolate.</strong> Bilinear combines the four nearest neighbors weighted by fractional offsets; it smooths jaggies compared to nearest but preserves edges fairly well.</li>
        <li><strong>Validity mask.</strong> We write 1 into <code>alpha</code> only when (x,y) lies inside the source bounds. This mask becomes crucial when blending multiple views.</li>
        <li><strong>Offsets.</strong> <code>offset_xy</code> lets us place warped content with negative coordinates onto a positive‑indexed array. For single‑view rectification it’s usually (0,0); for mosaics it’s often nonzero.</li>
        <li><strong>Quality tips.</strong> (a) If aliasing appears on fine patterns, pre‑blur slightly before warping. (b) If borders look dark, ensure out‑of‑bounds samples return <code>ok=False</code> rather than zeros that get averaged in.</li>
      </ol>

      <div class="grid cols-2">
        <figure><img src="images_A/a3_rect1_warp_nn.jpg" alt="Rectification – poster (NN)"><figcaption>Poster rectified (nearest neighbor).</figcaption></figure>
        <figure><img src="images_A/a3_rect1_warp_bilinear.jpg" alt="Rectification – poster (bilinear)"><figcaption>Poster rectified (bilinear) — fewer jaggies.</figcaption></figure>
        <figure><img src="images_A/a3_rect2_warp_nn.jpg" alt="Rectification – tiles (NN)"><figcaption>Tiles rectified (nearest neighbor).</figcaption></figure>
        <figure><img src="images_A/a3_rect2_warp_bilinear.jpg" alt="Rectification – tiles (bilinear)"><figcaption>Tiles rectified (bilinear) — smoother lines.</figcaption></figure>
      </div>
      <p class="callout"><strong>Takeaway.</strong> Nearest is fast but jaggy; bilinear trades a tiny bit of sharpness for much smoother edges. For mosaics you can consider bicubic or Lanczos, but bilinear is typically sufficient.</p>
    </section>

    <section id="a4">
      <h2>A.4 – Panoramic Mosaics (Project 3A)</h2>
      <p>
        We register all images to a single reference, estimate a canvas that fits every warped footprint, render each image with inverse warping, and <strong>feather‑blend</strong> overlaps so seams fade away.
      </p>

      <h3>Implementation — Canvas sizing & Feathered blending</h3>
      <pre class="code">def image_corners_wh(width: int, height: int) -> np.ndarray:
    return np.array([[0, 0], [width - 1, 0], [width - 1, height - 1], [0, height - 1]], dtype=np.float64)

def warp_footprint_bbox(width: int, height: int, H_map: np.ndarray):
    corners = image_corners_wh(width, height)
    warped  = apply_homography_points(corners, H_map)
    xs, ys  = warped[:, 0], warped[:, 1]
    if np.all(np.isnan(xs)) or np.all(np.isnan(ys)):
        return 0.0, 0.0, 0.0, 0.0
    return np.nanmin(xs), np.nanmax(xs), np.nanmin(ys), np.nanmax(ys)

def global_canvas_from_maps(image_shapes: List[Tuple[int, int]], H_maps: List[np.ndarray]):
    xs_min, xs_max, ys_min, ys_max = [], [], [], []
    for (H, W), Hc in zip(image_shapes, H_maps):
        xmin, xmax, ymin, ymax = warp_footprint_bbox(W, H, Hc)
        xs_min.append(xmin); xs_max.append(xmax)
        ys_min.append(ymin); ys_max.append(ymax)
    Xmin, Xmax = np.floor(min(xs_min)), np.ceil(max(xs_max))
    Ymin, Ymax = np.floor(min(ys_min)), np.ceil(max(ys_max))
    canvas_w = int(max(1, Xmax - Xmin))
    canvas_h = int(max(1, Ymax - Ymin))
    offset   = (-int(Xmin), -int(Ymin))
    return canvas_h, canvas_w, offset

def center_falloff_weights(alpha: np.ndarray) -> np.ndarray:
    H, W = alpha.shape
    y = np.arange(H)[:, None]
    x = np.arange(W)[None, :]
    d_left, d_right = x, (W - 1 - x)
    d_top,  d_bot   = y, (H - 1 - y)
    d = np.minimum(np.minimum(d_left, d_right), np.minimum(d_top, d_bot))
    d = d * alpha
    m = d.max() if d.max() > 0 else 1.0
    return d / m

def feather_blend(images: List[np.ndarray], alphas: List[np.ndarray]) -> np.ndarray:
    H, W, C = images[0].shape
    accum = np.zeros((H, W, C), dtype=np.float32)
    wsum  = np.zeros((H, W, 1), dtype=np.float32)
    for img, a in zip(images, alphas):
        w = center_falloff_weights(a) * (a > 0).astype(np.float32)
        w = w[..., None]
        accum += w * img
        wsum  += w
    wsum[wsum == 0] = 1.0
    return accum / wsum</pre>

      <h3>Walkthrough (full pipeline)</h3>
      <ol>
        <li><strong>Choose a reference.</strong> Pick the most central or sharpest frame as the origin (identity homography). Estimate <em>H</em> for every other image that maps it into this reference frame (manual correspondences in 3A).</li>
        <li><strong>Predict footprints.</strong> Warp each image’s four corners using its <em>H</em>. Taking global min/max over x and y gives the canvas bounds; the negative mins become the <code>offset</code>.</li>
        <li><strong>Render with masks.</strong> For each image, call <code>inverse_warp_to_canvas</code> to get an RGB array and an <code>alpha</code> mask indicating valid pixels. Store both.</li>
        <li><strong>Feather weights.</strong> Convert <code>alpha</code> to a smooth weight that decays near borders so overlaps blend gently instead of creating hard seams. (For even better results, multi‑band blending is a drop‑in upgrade.)</li>
        <li><strong>Accumulate & normalize.</strong> Sum <code>w * I</code> across all images and divide by the sum of weights per pixel. Pixels that only one image covers get weight 1; overlaps become a weighted average.</li>
        <li><strong>Practical tips.</strong> (a) Render the reference frame last or give it slightly higher weights to anchor exposure. (b) If exposures differ a lot, do a quick per‑image gain match in the overlap region before blending.</li>
      </ol>

      <div class="grid cols-3">
        <figure class="wide"><img src="images_A/a4_mosaic1_final.jpg" alt="3A mosaic 1"><figcaption>3A · Mosaic 1.</figcaption></figure>
        <figure class="wide"><img src="images_A/a4_mosaic2_final.jpg" alt="3A mosaic 2"><figcaption>3A · Mosaic 2.</figcaption></figure>
        <figure class="wide"><img src="images_A/a4_mosaic3_final.jpg" alt="3A mosaic 3"><figcaption>3A · Mosaic 3.</figcaption></figure>
      </div>
    </section>
    <!-- ===================== end 3A content ===================== -->

    <!-- ===================== 3B CONTENT ===================== -->
    <section id="b1">
      <h2>B.1 – Harris Corner Detection</h2>
      <p>
        We detect corners with a Harris interest point operator and optionally refine the set with <em>Adaptive Non-Maximal Suppression</em> (ANMS).
      </p>
      <div class="grid cols-4">
        <figure>
          <img src="images/set1_harris_A.jpg" alt="Set 1 – Image A Harris corners" />
          <figcaption>Set 1 · Image A · Harris corners.</figcaption>
        </figure>
        <figure>
          <img src="images/set1_harris_B.jpg" alt="Set 1 – Image B Harris corners" />
          <figcaption>Set 1 · Image B · Harris corners.</figcaption>
        </figure>
        <figure>
          <img src="images/set1_anms_A.jpg" alt="Set 1 – Image A ANMS corners" />
          <figcaption>Set 1 · Image A · ANMS-selected corners.</figcaption>
        </figure>
        <figure>
          <img src="images/set1_anms_B.jpg" alt="Set 1 – Image B ANMS corners" />
          <figcaption>Set 1 · Image B · ANMS-selected corners.</figcaption>
        </figure>
      </div>

      <p><strong>Intuition.</strong> A corner is a spot where intensity changes strongly in <em>both</em> directions (think: a checkerboard corner vs. a flat wall). Harris measures how much the image varies around each pixel. High scores indicate good, repeatable points that are easy to re‑find in another photo of the same scene.</p>
      <p><strong>Why ANMS?</strong> Raw detection can cluster many points in one small area and ignore others. ANMS keeps the strongest points while spreading them out spatially, which improves coverage and makes the later matching step more stable.</p>

      <h3>Implementation (Harris + ANMS)</h3>
      <pre class="code">def harris_corner_response(gray: np.ndarray, k_coeff: float = harris_k_coeff, window_size: int = harris_window_size) -> np.ndarray:
    sobel_x = np.array([[1, 0, -1], [2, 0, -2], [1, 0, -1]], dtype=np.float32)
    sobel_y = np.array([[1, 2, 1], [0, 0, 0], [-1, -2, -1]], dtype=np.float32)
    grad_x = conv2d_same(gray, sobel_x)
    grad_y = conv2d_same(gray, sobel_y)
    grad_xx, grad_yy, grad_xy = grad_x * grad_x, grad_y * grad_y, grad_x * grad_y
    gauss = gaussian_kernel_2d(window_size if window_size % 2 else window_size + 1, sigma=1.0)
    sum_xx = conv2d_same(grad_xx, gauss)
    sum_yy = conv2d_same(grad_yy, gauss)
    sum_xy = conv2d_same(grad_xy, gauss)
    determinant = sum_xx * sum_yy - sum_xy * sum_xy
    trace = sum_xx + sum_yy
    response = determinant - k_coeff * (trace ** 2)
    r_min, r_max = float(response.min()), float(response.max())
    if r_max > r_min:
        return (response - r_min) / (r_max - r_min)
    return np.zeros_like(response)

def nms_local_maxima(harris_norm: np.ndarray, threshold_relative: float = harris_threshold_relative):
    height, width = harris_norm.shape
    threshold_value = threshold_relative * float(harris_norm.max())
    keep_mask = np.zeros_like(harris_norm, dtype=bool)
    for y in range(1, height - 1):
        for x in range(1, width - 1):
            value = harris_norm[y, x]
            if value < threshold_value:
                continue
            window = harris_norm[y - 1:y + 2, x - 1:x + 2]
            if value == window.max() and np.count_nonzero(window == value) == 1:
                keep_mask[y, x] = True
    ys, xs = np.where(keep_mask)
    vals = harris_norm[ys, xs]
    return xs.astype(np.float32), ys.astype(np.float32), vals.astype(np.float32)

def adaptive_nms(x_coords: np.ndarray, y_coords: np.ndarray, strengths: np.ndarray, top_k: int = max_keypoints_after_anms):
    count = len(x_coords)
    if count == 0:
        return x_coords, y_coords, strengths
    order = np.argsort(-strengths)
    x_coords, y_coords, strengths = x_coords[order], y_coords[order], strengths[order]
    radii_sq = np.full(count, np.inf, dtype=np.float32)
    for i in range(1, count):
        dx = x_coords[:i] - x_coords[i]
        dy = y_coords[:i] - y_coords[i]
        dist2 = dx * dx + dy * dy
        radii_sq[i] = float(np.min(dist2)) if i > 0 else np.inf
    radii_sq[0] = np.inf
    selected = np.argsort(-radii_sq)[:min(top_k, count)]
    return x_coords[selected], y_coords[selected], strengths[selected]</pre>
      <p class="callout"><strong>Walkthrough.</strong> <em>harris_corner_response</em> builds x/y gradients with Sobel filters, aggregates squared terms with a Gaussian window (approximating the structure tensor), and returns a normalized Harris score per pixel. <em>nms_local_maxima</em> keeps only strict 3×3 peaks above a relative threshold. <em>adaptive_nms</em> then sorts by strength and assigns each point a suppression radius to the nearest stronger point; we keep the largest radii so corners are spatially spread out and stable for matching.</p>
    </section>

    <section id="b2">
      <h2>B.2 – Feature Descriptor Extraction</h2>
      <p>
        Around each selected corner, we sample a <strong>40×40</strong> window (blurred / downsampled) to produce a normalized <strong>8×8</strong> grayscale descriptor (bias–gain normalized).
        Below are some of the <em>Set 1</em> overlays (40×40 boxes with center points at some corners) and the corresponding 8×8 descriptor montages.
      </p>

      <div class="grid cols-4">
        <figure class="wide">
          <img src="images/set1_desc_A_patch_boxes.jpg" alt="Set 1 – Image A patch boxes + centers" />
          <figcaption>Set 1 · Image A · 40×40 patch boxes (inlier corners) + center points.</figcaption>
        </figure>
        <figure class="wide">
          <img src="images/set1_desc_B_patch_boxes.jpg" alt="Set 1 – Image B patch boxes + centers" />
          <figcaption>Set 1 · Image B · 40×40 patch boxes (inlier corners) + center points.</figcaption>
        </figure>
        <figure class="wide">
          <img src="images/set1_desc_A_montage.png" alt="Set 1 – Image A montage of 8×8 descriptors" />
          <figcaption>Set 1 · Image A · Montage of 8×8 normalized descriptor patches.</figcaption>
        </figure>
        <figure class="wide">
          <img src="images/set1_desc_B_montage.png" alt="Set 1 – Image B montage of 8×8 descriptors" />
          <figcaption>Set 1 · Image B · Montage of 8×8 normalized descriptor patches.</figcaption>
        </figure>
      </div>

      <h3>Implementation (Patch → 8×8 → z‑score)</h3>
      <pre class="code">def extract_square_patch(gray: np.ndarray, center_x: float, center_y: float, size: int = 40) -> np.ndarray:
    height, width = gray.shape
    half = size // 2
    x0, y0 = int(round(center_x)) - half, int(round(center_y)) - half
    x1, y1 = x0 + size, y0 + size
    if x0 < 0 or y0 < 0 or x1 > width or y1 > height:
        return None
    return gray[y0:y1, x0:x1].copy()

def resize_patch_to_8x8(patch_40: np.ndarray) -> np.ndarray:
    patch_uint8 = (np.clip(patch_40, 0, 1) * 255).astype(np.uint8)
    image = Image.fromarray(patch_uint8)
    image8 = image.resize((8, 8), resample=Image.BILINEAR)
    return np.asarray(image8).astype(np.float32) / 255.0

def normalize_descriptor_vector(patch_8x8: np.ndarray) -> np.ndarray:
    vector = patch_8x8.flatten().astype(np.float32)
    mean_value = float(np.mean(vector))
    std_value = float(np.std(vector))
    if std_value < 1e-6:
        std_value = 1.0
    return (vector - mean_value) / std_value

def build_patch_descriptors(gray: np.ndarray, x_coords: np.ndarray, y_coords: np.ndarray):
    descriptor_list, kept_x, kept_y, display_patches = [], [], [], []
    for x_val, y_val in zip(x_coords, y_coords):
        patch = extract_square_patch(gray, x_val, y_val, size=40)
        if patch is None:
            continue
        patch8 = resize_patch_to_8x8(patch)
        descriptor = normalize_descriptor_vector(patch8)
        descriptor_list.append(descriptor)
        kept_x.append(x_val)
        kept_y.append(y_val)
        display_patches.append(normalize_patch_for_display(patch8))
    if len(descriptor_list) == 0:
        return np.zeros((0, 64), dtype=np.float32), np.array([]), np.array([]), np.zeros((0, 8, 8), dtype=np.float32)
    return np.vstack(descriptor_list), np.array(kept_x), np.array(kept_y), np.stack(display_patches, axis=0)</pre>
      <p class="callout"><strong>Walkthrough.</strong> We crop a 40×40 window centered on each corner (skip if it would go out of bounds), bilinearly resize to 8×8 to reduce noise and standardize size, then flatten and z‑score the 64 pixels to remove brightness/contrast bias. The helper returns both the numeric descriptors (for matching) and display‑normalized patches (for the montages you see above).</p>
      <p class="callout">
        Each corner produces a 40×40 patch (bounds-checked), which is bilinearly resized to 8×8 and z-score normalized to suppress illumination bias and gain differences.
      </p>
      <p><strong>Step‑by‑step (from scratch):</strong></p>
      <ol>
        <li>For each selected corner, crop a 40×40 grayscale window centered on it.</li>
        <li>Downsample that window to 8×8 using bilinear interpolation. This reduces noise and makes all descriptors the same size.</li>
        <li>Normalize the 64 values: subtract the mean and divide by the standard deviation (z‑score). This makes descriptors less sensitive to brightness/exposure.</li>
        <li>Optionally visualize: draw 40×40 boxes on the original images and show grids of the 8×8 patches (shown above for Set 1).</li>
      </ol>
    </section>

    <section id="b3">
      <h2>B.3 – Feature Matching</h2>
      <p>
        We compute nearest neighbors in descriptor space and apply Lowe’s <em>ratio test</em> to keep reliable correspondences.
      </p>
      <div class="grid cols-2">
        <figure class="wide">
          <img src="images/set1_matches_filtered.jpg" alt="Set 1 – Filtered matches after Lowe ratio test" />
          <figcaption>Set 1 · Filtered matches (post ratio test).</figcaption>
        </figure>
        <figure>
          <img src="images/set2_matches_filtered.jpg" alt="Set 2 – Filtered matches after Lowe ratio test" />
          <figcaption>Set 2 · Filtered matches (post ratio test).</figcaption>
        </figure>
        <figure class="wide">
          <img src="images/set3_matches_filtered.jpg" alt="Set 2 – Filtered matches after Lowe ratio test" />
          <figcaption>Set 2 · Filtered matches (post ratio test).</figcaption>
        </figure>
      </div>

      <p><strong>What is a “match”?</strong> After we convert patches to 64‑D vectors, we can measure how similar two patches are by their Euclidean distance (small = similar). For each left‑image descriptor we find its nearest neighbor in the right image.</p>
      <p><strong>Why the ratio test?</strong> Sometimes the best and second‑best matches are similarly good—an ambiguity that often means the match isn’t reliable. The ratio test accepts a pair only if “best” is noticeably better than “runner‑up”. This prunes many false pairs while keeping the real ones.</p>

      <h3>Implementation (Lowe ratio matching)</h3>
      <pre class="code">def match_with_lowe_ratio(descriptors_left: np.ndarray, descriptors_right: np.ndarray, ratio_threshold: float = lowe_ratio_threshold):
    matches = []
    if len(descriptors_left) == 0 or len(descriptors_right) == 0:
        return matches
    distances = np.sum((descriptors_left[:, None, :] - descriptors_right[None, :, :]) ** 2, axis=2)
    for idx_left in range(descriptors_left.shape[0]):
        row = distances[idx_left]
        order = np.argsort(row)
        idx_right_best = int(order[0])
        idx_right_next = int(order[1]) if len(order) > 1 else int(order[0])
        best_dist = float(row[idx_right_best])
        next_dist = float(row[idx_right_next])
        if next_dist == 0:
            continue
        if best_dist / next_dist < ratio_threshold:
            matches.append((idx_left, idx_right_best, best_dist, next_dist))
    return matches</pre>
      <p class="callout"><strong>Walkthrough.</strong> We compute the dense left‑to‑right distance matrix once, then for each left descriptor pick the smallest (best) and second‑smallest (runner‑up) distances. A match is kept only if <code>best/next &lt; ratio_threshold</code> (default 0.7), which filters out ambiguous pairs that are likely false.</p>
      <p class="callout">
        For each left descriptor, we take the best and second-best right matches by L2 distance and accept the match if the best/second-best ratio is below 0.7.
      </p>
    </section>

    <section id="b4">
      <h2>B.4 – RANSAC for Robust Homography & Mosaics</h2>
      <p>
        Using 4-point RANSAC, we estimate a homography from inlier matches, warp one image into the other’s frame, and blend to form a mosaic.
      </p>
      <div class="grid cols-2">
        <figure class="wide">
          <img src="images/set1_ransac_inliers.jpg" alt="Set 1 – RANSAC inliers visualization" />
          <figcaption>Set 1 · RANSAC inliers (green) vs outliers (red).</figcaption>
        </figure>
      </div>

      <p><strong>What is a homography?</strong> It’s a 3×3 matrix that maps points from one flat view to another (e.g., two photos of the same wall taken from different positions). With enough correct point pairs, we can estimate this matrix and use it to align entire images.</p>
      <p><strong>Why RANSAC?</strong> Some matches are wrong. RANSAC repeatedly picks 4 random pairs, computes a candidate homography, and counts how many matches agree (inliers). The candidate with the most inliers “wins”, and we refit using just those inliers for accuracy.</p>
      <p><strong>From math to pixels:</strong> Once we have the homography, we <em>inverse‑warp</em> the moving image into a shared canvas and blend overlaps with soft weights so seams fade away.</p>

      <h3>Implementation (RANSAC + inverse warp)</h3>
      <pre class="code">def ransac_fit_homography(src_xy: np.ndarray, dst_xy: np.ndarray, max_iters: int = ransac_max_iterations, eps: float = ransac_inlier_epsilon):
    best_inlier_mask = None
    best_homography = None
    count = src_xy.shape[0]
    if count < 4:
        return None, np.array([], dtype=bool)
    for _ in range(max_iters):
        sample_idx = np.random.choice(count, 4, replace=False)
        candidate = dlt_homography(src_xy[sample_idx], dst_xy[sample_idx])
        predicted = apply_homography_points(src_xy, candidate)
        error = np.sqrt(np.sum((predicted - dst_xy) ** 2, axis=1))
        inliers = error < eps
        if best_inlier_mask is None or np.count_nonzero(inliers) > np.count_nonzero(best_inlier_mask):
            best_inlier_mask = inliers
            best_homography = candidate
    if best_inlier_mask is not None and np.count_nonzero(best_inlier_mask) >= 4:
        best_homography = dlt_homography(src_xy[best_inlier_mask], dst_xy[best_inlier_mask])
    else:
        best_homography = None
        best_inlier_mask = np.zeros(count, dtype=bool)
    return best_homography, best_inlier_mask

def inverse_warp_to_canvas(src_image: np.ndarray, inverse_homography: np.ndarray, canvas_height: int, canvas_width: int, canvas_offset_xy: Tuple[int, int], mode: str = "bilinear"):
    offset_x, offset_y = canvas_offset_xy
    channels = src_image.shape[2] if src_image.ndim == 3 else 1
    canvas = np.zeros((canvas_height, canvas_width, channels), dtype=np.float32)
    alpha = np.zeros((canvas_height, canvas_width), dtype=np.float32)
    sampler = sample_bilinear if mode == "bilinear" else sample_nearest
    h0, h1, h2 = inverse_homography[0], inverse_homography[1], inverse_homography[2]
    for Y in range(canvas_height):
        for X in range(canvas_width):
            Xc, Yc = float(X), float(Y)
            w = h2[0] * (Xc - offset_x) + h2[1] * (Yc - offset_y) + h2[2]
            if abs(w) < 1e-12:
                continue
            x = (h0[0] * (Xc - offset_x) + h0[1] * (Yc - offset_y) + h0[2]) / w
            y = (h1[0] * (Xc - offset_x) + h1[1] * (Yc - offset_y) + h1[2]) / w
            value, ok = sampler(src_image, x, y)
            if ok:
                canvas[Y, X] = value
                alpha[Y, X] = 1.0
    return canvas, alpha</pre>
      <p class="callout"><strong>Walkthrough.</strong> <em>ransac_fit_homography</em> loops over random 4‑point samples to propose a homography via DLT, labels inliers by reprojection error &lt; <code>eps</code>, and keeps the proposal with the largest consensus before refitting on all inliers. <em>inverse_warp_to_canvas</em> then maps every output pixel back into the source via the inverse homography and samples (bilinear by default); we also produce an <code>alpha</code> mask so overlapping images can be feather‑blended smoothly.</p>
      <p class="callout">
        We randomly sample 4 correspondences to fit H (DLT), score inliers by reprojection error, keep the best consensus, refit on all inliers, then inverse-warp and feather-blend onto a common canvas.
      </p>
    </section>
    <!-- ===================== end 3B content ===================== -->

    <section id="results">
      <h2>Results Gallery</h2>
      <p>Stitched panoramas from the full automatic pipeline (3B). Click to view full resolution.</p>
      <div class="grid cols-3">
        <figure class="wide"><a href="images/set1_mosaic_auto.jpg" target="_blank" rel="noopener"><img src="images/set1_mosaic_auto.jpg" alt="Gallery mosaic – Set 1" /></a><figcaption>Automatic mosaic · Set 1.</figcaption></figure>
        <figure class="wide"><a href="images/set2_mosaic_auto.jpg" target="_blank" rel="noopener"><img src="images/set2_mosaic_auto.jpg" alt="Gallery mosaic – Set  2" /></a><figcaption>Automatic mosaic · Set 2.</figcaption></figure>
        <figure class="wide"><a href="images/set3_mosaic_auto.jpg" target="_blank" rel="noopener"><img src="images/set3_mosaic_auto.jpg" alt="Gallery mosaic – Set 3" /></a><figcaption>Automatic mosaic · Set 3.</figcaption></figure>
      </div>
    </section>

    <section id="implementation">
      <h2>Implementation Notes</h2>
      <p><strong>Parameters (plain English).</strong> Harris threshold trades recall vs. noise; ANMS count controls spatial spread; Lowe ratio trades precision vs. recall; RANSAC epsilon is the inlier pixel error; iterations control robustness to outliers.</p>
      <pre class="code">max_keypoints_after_anms = 750
harris_k_coeff = 0.04
harris_window_size = 3
harris_threshold_relative = 0.01
lowe_ratio_threshold = 0.7
ransac_max_iterations = 2000
ransac_inlier_epsilon = 3.0</pre>
    </section>

    <section id="references">
      <h2>References</h2>
      <ul>
        <li>Brown, Szeliski, Winder. <em>Multi-Image Matching using Multi-Scale Oriented Patches (MOPS)</em>.</li>
        <li>Harris & Stephens. <em>A Combined Corner and Edge Detector</em>.</li>
        <li>Lowe. <em>Distinctive Image Features from Scale-Invariant Keypoints</em>.</li>
      </ul>
      <p class="footer">Last updated: <span id="last-updated">—</span></p>
    </section>

    <footer class="footer">
      <p>© 2025 Eduardo Cortes. This page is part of CS180/280A coursework. Images are my own unless otherwise noted.</p>
      <p class="ai-ack">AI acknowledgment: I used ChatGPT (GPT-5 Thinking) to help draft this HTML/CSS template and organize content. All experiments, code, images, offsets, and write-up are my own; I reviewed and verified technical details.</p>
    </footer>
  </div>

  <script>
    const span = document.getElementById('last-updated');
    if (span) {
      const d = new Date();
      const fmt = new Intl.DateTimeFormat([], { year: 'numeric', month: 'short', day: '2-digit' }).format(d);
      span.textContent = fmt;
    }
  </script>
</body>
</html>
