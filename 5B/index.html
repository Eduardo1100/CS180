<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>CS180 Project 5B: Single-step Denoising UNet + Flow Matching</title>
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="styles.css" />
</head>

<body>
  <header class="topbar">
    <div class="topbar-inner">
      <div class="brand">
        <div class="brand-title">CS180 Project 5B</div>
        <div class="brand-subtitle">Single-step Denoising UNet · Flow Matching · Time + Class Conditioning</div>
      </div>

      <nav class="nav">
        <a href="#overview">Overview</a>
        <a href="#part1">Part 1</a>
        <a href="#part2">Part 2</a>
        <a href="#impl-notes">Implementation Notes</a>
        <a href="#refs">References</a>
      </nav>
    </div>
  </header>

  <main class="container">

    <!-- ===================== OVERVIEW ===================== -->
    <section id="overview" class="section">
      <h1>Overview</h1>

      <p>
        This project builds image generation in two stages:
        first, we train a UNet to do <b>single-step denoising</b> on MNIST (remove one fixed amount of Gaussian noise).
        Then, we upgrade the idea into <b>Flow Matching</b>, where the model learns a <i>velocity field</i> that can
        transform pure noise into digits through many small steps.
      </p>

      <p class="callout">
        <b>Intuition.</b> If diffusion models are “fog removal,” flow matching is “learning a wind field.”
        Each pixel gets a tiny push, and repeating those pushes moves a random cloud into a digit-shaped cluster.
      </p>

      <div class="grid cols-2">
        <div class="card">
          <h3>What you’ll see</h3>
          <ul>
            <li>How noise changes MNIST digits across different σ values</li>
            <li>A UNet that denoises σ = 0.5 in one shot</li>
            <li>OOD tests: denoise noise levels the model never trained on</li>
            <li>“Pure noise denoising” and why it produces blurry digit-like averages</li>
            <li>Flow Matching training + sampling (time-conditioned)</li>
            <li>Class-conditioning + classifier-free guidance for controllable generation</li>
          </ul>
        </div>
        <div class="card">
          <h3>Key idea in one sentence</h3>
          <p>
            We train a UNet to predict either:
            <b>(A)</b> a clean image from a noisy one (single-step denoising), or
            <b>(B)</b> a <i>flow</i> that tells us how to move from noise toward data (flow matching).
          </p>
          <p class="muted">
            Dataset: MNIST (28×28 grayscale). Models: UNet variants with skip connections.
          </p>
        </div>
      </div>
    </section>

    <!-- ===================== PART 1 ===================== -->
    <section id="part1" class="section">
      <h1>Part 1: Training a Single-step Denoising UNet</h1>

      <!-- 1.1 -->
      <section class="subsection">
        <h2>1.1 Implementing the UNet</h2>

        <p>
          A UNet is a neural network shaped like a “U”: the left side compresses the image into a smaller representation,
          the middle mixes information, and the right side expands back to full resolution. The magic trick is the
          <b>skip connections</b>, which copy high-resolution details from the encoder to the decoder.
        </p>

        <p class="callout">
          <b>Why skip connections matter.</b>
          If we only compress and expand, we risk losing sharp edges and strokes. Skip connections act like “memory lanes”
          for fine details that are hard to reconstruct from a tiny bottleneck.
        </p>

        <h3>Core building blocks (Conv / Down / Up / Flatten / Unflatten)</h3>
        <pre class="code"><code># --- Simple ops used everywhere in the UNet ---

class Conv(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.net = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(out_channels),
            nn.GELU(),
        )

    def forward(self, x):
        return self.net(x)


class DownConv(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()
        # kernel=4, stride=2 halves H,W
        self.net = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(out_channels),
            nn.GELU(),
        )

    def forward(self, x):
        return self.net(x)


class UpConv(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()
        # transpose conv doubles H,W
        self.net = nn.Sequential(
            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(out_channels),
            nn.GELU(),
        )

    def forward(self, x):
        return self.net(x)


class Flatten(nn.Module):
    def __init__(self):
        super().__init__()
        # 7x7 -> 1x1 by averaging each channel
        self.pool = nn.AvgPool2d(kernel_size=7)

    def forward(self, x):
        return self.pool(x)


class Unflatten(nn.Module):
    def __init__(self, in_channels):
        super().__init__()
        # 1x1 -> 7x7 using a transposed conv
        self.unflatten = nn.ConvTranspose2d(
            in_channels, in_channels, kernel_size=7, stride=7, padding=0
        )

    def forward(self, x):
        return self.unflatten(x)</code></pre>

        <p class="walkthrough">
          <b>Walkthrough.</b>
          <span class="mono">Conv</span> keeps the spatial size (28×28 stays 28×28) but changes the channel count, which is like
          “repainting” the image into a new feature space. <span class="mono">DownConv</span> uses stride 2 to shrink the grid,
          turning local patterns into a more compact description. <span class="mono">UpConv</span> reverses that shrink, expanding
          the grid back. <span class="mono">Flatten</span> is not flattening into a vector; it’s averaging 7×7 into 1×1 per channel,
          creating a tiny “global summary.” <span class="mono">Unflatten</span> expands that summary into a 7×7 map, so the decoder
          can start rebuilding detail.
        </p>

        <h3>Unconditional UNet (skip connections + concatenation)</h3>
        <pre class="code"><code>class UnconditionalUNet(nn.Module):
    def __init__(self, in_channels, num_hiddens):
        super().__init__()
        D = num_hiddens

        # Encoder
        self.in_conv = ConvBlock(in_channels, D)  # 28x28
        self.down1   = DownBlock(D, D)            # 14x14
        self.down2   = DownBlock(D, 2*D)          # 7x7

        # Bottleneck
        self.flatten   = Flatten()                # 7x7 -> 1x1
        self.mid       = ConvBlock(2*D, 2*D)      # 1x1
        self.unflatten = Unflatten(2*D)           # 1x1 -> 7x7

        # Decoder
        self.dec7   = ConvBlock(4*D, 2*D)         # concat with down2
        self.up1    = UpBlock(2*D, D)             # 14x14
        self.dec14  = ConvBlock(2*D, D)           # concat with down1
        self.up2    = UpBlock(D, D)               # 28x28
        self.dec28  = ConvBlock(2*D, D)           # concat with in_conv

        self.out_conv = nn.Conv2d(D, in_channels, kernel_size=1)

    def forward(self, x):
        x0 = self.in_conv(x)             # (N, D, 28, 28)
        x1 = self.down1(x0)              # (N, D, 14, 14)
        x2 = self.down2(x1)              # (N, 2D, 7, 7)

        h  = self.flatten(x2)            # (N, 2D, 1, 1)
        h  = self.mid(h)                 # (N, 2D, 1, 1)
        h  = self.unflatten(h)           # (N, 2D, 7, 7)

        h  = torch.cat([h, x2], dim=1)   # (N, 4D, 7, 7)
        h  = self.dec7(h)                # (N, 2D, 7, 7)

        h  = self.up1(h)                 # (N, D, 14, 14)
        h  = torch.cat([h, x1], dim=1)   # (N, 2D, 14, 14)
        h  = self.dec14(h)               # (N, D, 14, 14)

        h  = self.up2(h)                 # (N, D, 28, 28)
        h  = torch.cat([h, x0], dim=1)   # (N, 2D, 28, 28)
        h  = self.dec28(h)               # (N, D, 28, 28)

        return self.out_conv(h)          # (N, 1, 28, 28)</code></pre>

        <p class="walkthrough">
          <b>Walkthrough.</b>
          The encoder produces three “snapshots” of the same image at different resolutions: 28×28 (fine detail),
          14×14 (mid-level structure), and 7×7 (global structure). The bottleneck compresses to 1×1, which forces the
          network to form a summary of “what digit is this and where are the strokes.” Then the decoder expands back.
          At each decoder stage, we concatenate (channel-wise) the matching encoder snapshot. That concatenation is a direct
          injection of detail: instead of guessing edges from scratch, the decoder receives them explicitly and learns how
          to use them to reconstruct the clean image.
        </p>
      </section>

      <!-- 1.2 -->
      <section class="subsection">
        <h2>1.2 Training Data: Adding Noise to MNIST</h2>

        <p>
          We generate training pairs <span class="mono">(z, x)</span>, where:
          <span class="mono">x</span> is a clean MNIST digit and
          <span class="mono">z = x + σ·ε</span> with <span class="mono">ε ~ N(0, I)</span>.
          For visualization we sweep σ across several values, but for training we use σ = 0.5.
        </p>

        <figure class="wide">
          <img src="images/p12_noising.png" alt="Noising process across sigmas for digits 0-9">
          <figcaption>
            <b>Deliverable: p12_noising.png.</b> Each row is a digit (0–9), each column is a different σ.
            As σ increases, the signal becomes harder to recognize because noise dominates the pixels.
          </figcaption>
        </figure>

        <pre class="code"><code>def add_gaussian_noise(x, sigma):
    eps = torch.randn_like(x)
    z = x + sigma * eps
    return z</code></pre>

        <p class="walkthrough">
          <b>Walkthrough.</b>
          This is the entire corruption process. <span class="mono">torch.randn_like</span> creates random noise with the same
          shape as the image, meaning one independent noise value per pixel. Multiplying by σ controls the noise strength.
          Adding it to the clean image gives a “noisy observation.” During training, we re-sample ε each iteration so the
          network does not memorize one fixed noisy version of each digit.
        </p>
      </section>

      <!-- 1.2.1 -->
      <section class="subsection">
        <h2>1.2.1 Training the Single-step Denoiser (σ = 0.5)</h2>

        <p>
          We train the UNet to predict the clean image in one shot:
          <span class="mono">Dθ(z) ≈ x</span>. The loss is mean squared error (MSE),
          which measures pixel-by-pixel reconstruction quality.
        </p>

        <p class="callout">
          <b>Important detail.</b> We add noise <i>inside</i> the training loop (not precomputed),
          so each epoch uses fresh noise. That makes the model generalize instead of memorizing.
        </p>

        <pre class="code"><code>model = UnconditionalUNet(in_channels=1, num_hiddens=128).to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)
criterion = nn.MSELoss()

for epoch in range(5):
    model.train()
    for images, _ in train_loader:
        images = images.to(device)              # clean x
        noised = add_gaussian_noise(images, 0.5)  # z = x + 0.5 * eps

        pred = model(noised)                    # Dθ(z)
        loss = criterion(pred, images)          # ||Dθ(z) - x||^2

        optimizer.zero_grad(set_to_none=True)
        loss.backward()
        optimizer.step()</code></pre>

        <p class="walkthrough">
          <b>Walkthrough.</b>
          We first create the model and choose Adam, a standard optimizer that adapts step sizes per parameter.
          Each minibatch provides clean images <span class="mono">x</span>. We corrupt them to create <span class="mono">z</span>.
          The UNet predicts a reconstruction <span class="mono">Dθ(z)</span>, and MSE compares it to the original clean image.
          Backpropagation computes how each weight contributed to the error, and Adam updates weights to reduce future error.
          Doing this repeatedly teaches the UNet to “undo” the specific corruption process used at training time.
        </p>

        <figure class="wide">
          <img src="images/p121_loss.png" alt="Training loss curve for sigma=0.5 denoiser">
          <figcaption>
            <b>Deliverable: p121_loss.png.</b> Training loss over iterations for single-step denoising at σ=0.5.
          </figcaption>
        </figure>

        <figure class="wide">
          <img src="images/p121_samples.png" alt="Epoch 1 and Epoch 5 denoising samples">
          <figcaption>
            <b>Deliverable: p121_samples.png.</b> For each digit (0–9): clean → noisy (σ=0.5) → denoised prediction.
            Shown after epoch 1 and epoch 5 (using saved checkpoints).
          </figcaption>
        </figure>
      </section>

      <!-- 1.2.2 -->
      <section class="subsection">
        <h2>1.2.2 Out-of-Distribution Noise Levels (OOD)</h2>

        <p>
          Now we stress-test the model by giving it noise levels it did not train on. We keep the same digits and vary σ.
          The goal is to see if the model learned a “general denoising idea” or only a single narrow mapping.
        </p>

        <figure class="wide">
          <img src="images/p122_epoch1_samples.png" alt="OOD denoising grid after epoch 1 checkpoint">
          <figcaption>
            <b>Deliverable: p122_epoch1_samples.png.</b> OOD denoising results after epoch 1 checkpoint.
          </figcaption>
        </figure>

        <figure class="wide">
          <img src="images/p122_epoch2_samples.png" alt="OOD denoising grid after epoch 5 checkpoint">
          <figcaption>
            <b>Deliverable: p122_epoch2_samples.png.</b> OOD denoising results after epoch 5 checkpoint.
          </figcaption>
        </figure>

        <p class="callout">
          <b>What to look for.</b> For σ smaller than training, denoising should be easy.
          For σ larger than training, the model often “hallucinates” plausible strokes because the input contains too little signal.
        </p>
      </section>

      <!-- 1.2.3 -->
      <section class="subsection">
        <h2>1.2.3 Denoising Pure Noise</h2>

        <p>
          In this experiment, the input is pure Gaussian noise, completely unrelated to the target image.
          That means the task is fundamentally ambiguous: many different digits could correspond to the same noise input distribution.
          Under MSE training, the model tends to learn an “average” digit-like output rather than a specific digit.
        </p>

        <pre class="code"><code># Pure noise input z ~ N(0,1), target is a real MNIST image x
for images, _ in train_loader:
    images = images.to(device)
    z = torch.randn_like(images)     # unrelated noise input
    pred = model(z)
    loss = mse(pred, images)</code></pre>

        <p class="walkthrough">
          <b>Walkthrough.</b>
          The key difference is that <span class="mono">z</span> no longer contains <i>any information</i> about the target image.
          The model is being asked to invert a mapping that does not exist. In that situation, MSE pushes the model toward outputs
          that reduce average error across many possible targets. Practically, that means blurry prototypes and faint digit-like strokes
          can appear, because the model learns statistical structure of MNIST rather than “recovering” a correct hidden image.
        </p>

        <figure class="wide">
          <img src="images/p123_pure_noise_loss.png" alt="Pure noise denoising training loss curve">
          <figcaption>
            <b>Deliverable: p123_pure_noise_loss.png.</b> Training loss when input is pure noise and target is MNIST.
          </figcaption>
        </figure>

        <div class="grid cols-2">
          <figure>
            <img src="images/p123_pure_noise_epoch1_samples.png" alt="Pure noise denoising samples epoch 1">
            <figcaption><b>Epoch 1.</b> Early outputs are often faint blobs or weak stroke fragments.</figcaption>
          </figure>
          <figure>
            <img src="images/p123_pure_noise_epoch5_samples.png" alt="Pure noise denoising samples epoch 5">
            <figcaption><b>Epoch 5.</b> Later outputs may look more “digit-like,” but still not tied to the input noise.</figcaption>
          </figure>
        </div>

        <p class="callout">
          <b>Why digit-like shapes appear.</b> Even though the input is meaningless, the model still sees MNIST targets during training.
          The easiest way to reduce MSE is to output “typical MNIST-looking pixels” (dark background, bright strokes in common locations).
          It’s learning the dataset’s average structure, not performing true recovery.
        </p>
      </section>
    </section>

    <!-- ===================== PART 2 ===================== -->
    <section id="part2" class="section">
      <h1>Part 2: Training a Flow Matching Model</h1>

      <p>
        Single-step denoising is useful but limited: it only learns one fixed noise strength.
        Flow matching trains a UNet to predict a <b>velocity</b> that moves a point along a line from noise to data.
        We sample:
      </p>

      <ul>
        <li><span class="mono">x0 ~ N(0, I)</span> (pure noise)</li>
        <li><span class="mono">x1 ~ data</span> (a real MNIST image)</li>
        <li><span class="mono">t ~ Uniform[0, 1]</span> (a random time)</li>
      </ul>

      <p>
        Then we define an interpolated point:
        <span class="mono">xt = (1 − t) x0 + t x1</span>.
        The true velocity is constant along this line:
        <span class="mono">u(xt, t) = x1 − x0</span>.
        The model learns to predict that velocity from <span class="mono">(xt, t)</span>.
      </p>

      <p class="callout">
        <b>Intuition.</b> Instead of “guessing the clean image,” we learn “how to move.”
        Sampling becomes repeatedly applying small moves, like taking many tiny steps uphill toward the data manifold.
      </p>

      <!-- 2.1 -->
      <section class="subsection">
        <h2>2.1 Adding Time Conditioning (FCBlock)</h2>

        <p>
          The model needs to know <i>where</i> it is along the path from noise to data, because early times should behave differently
          than late times. We inject scalar time <span class="mono">t</span> through a small MLP called <span class="mono">FCBlock</span>.
        </p>

        <pre class="code"><code>class FCBlock(nn.Module):
    def __init__(self, in_features, out_features):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(in_features, out_features),
            nn.GELU(),
            nn.Linear(out_features, out_features),
        )

    def forward(self, x):
        return self.net(x)</code></pre>

        <p class="walkthrough">
          <b>Walkthrough.</b>
          <span class="mono">FCBlock</span> is a tiny neural network that maps low-dimensional conditioning signals into feature vectors.
          The first linear layer expands the scalar time into a richer representation, GELU adds nonlinearity (so the mapping can bend),
          and the second linear layer allows a second “mixing pass.” This produces a conditioning vector that can modulate UNet features.
        </p>

        <h3>Time-conditioned modulation inside the UNet</h3>
        <pre class="code"><code># t is normalized to [0,1], shaped (N,1) before FCBlocks
t1 = fc1_t(t).view(N, 2*D, 1, 1)
t2 = fc2_t(t).view(N,   D, 1, 1)

h_unflatten = h_unflatten * t1   # modulate at 7x7
h_up1       = h_up1       * t2   # modulate at 14x14</code></pre>

        <p class="walkthrough">
          <b>Walkthrough.</b>
          We produce two time embeddings because the UNet has two key “decoder moments” where it benefits from knowing the time:
          right after the bottleneck (global structure decisions) and after the first upsample (mid-level stroke placement).
          Reshaping to <span class="mono">(N, C, 1, 1)</span> lets the time embedding broadcast across the whole spatial grid, meaning
          time acts like a per-channel gain knob: some feature channels should be louder early, others later.
        </p>
      </section>

      <!-- 2.2 -->
      <section class="subsection">
        <h2>2.2 Training the Time-conditioned Flow Matching UNet</h2>

        <pre class="code"><code># Sample x0 ~ N(0, I), t ~ U[0,1], build xt, and train uθ(xt, t) ≈ (x1 - x0)
x0 = torch.randn_like(x1)
t  = torch.rand(N, device=x1.device)

xt = (1 - t.view(N,1,1,1)) * x0 + t.view(N,1,1,1) * x1
u_target = x1 - x0

u_pred = unet(xt, t)
loss = ((u_pred - u_target) ** 2).mean()</code></pre>

        <p class="walkthrough">
          <b>Walkthrough.</b>
          We create a paired “start” (<span class="mono">x0</span>) and “end” (<span class="mono">x1</span>) for each training example.
          The random time <span class="mono">t</span> chooses a point between them. The model sees that intermediate point <span class="mono">xt</span>
          and must output the direction (velocity) that would carry it toward <span class="mono">x1</span>. Because the true line interpolation has
          constant velocity <span class="mono">x1 − x0</span>, this becomes a clean supervised learning problem: predict a target vector field.
        </p>

        <figure class="wide">
          <img src="images/p22_time_fm_loss.png" alt="Time-conditioned flow matching training loss curve">
          <figcaption>
            <b>Training curve.</b> <span class="mono">p22_time_fm_loss.png</span> for the time-conditioned flow matching model.
          </figcaption>
        </figure>
      </section>

      <!-- 2.3 -->
      <section class="subsection">
        <h2>2.3 Sampling from the Time-conditioned UNet</h2>

        <p>
          To generate an image, we start from noise and repeatedly apply the predicted velocity:
          <span class="mono">xt+Δ = xt + (1/T) · uθ(xt, t)</span>.
        </p>

        <pre class="code"><code>x = torch.randn(N, 1, 28, 28, device=device)
dt = 1.0 / T

for i in range(T):
    t = torch.full((N,), i / T, device=device)
    u = unet(x, t)
    x = x + dt * u</code></pre>

        <p class="walkthrough">
          <b>Walkthrough.</b>
          This loop is just numerical integration. Think of <span class="mono">u</span> as “the direction to move the pixels right now.”
          If we take one huge step, we overshoot. If we take many small steps (T steps), the image gradually morphs from random noise into a digit.
          The time <span class="mono">t</span> changes each step so the model can behave differently early vs late in the trajectory.
        </p>

        <div class="grid cols-3">
          <figure>
            <img src="images/p23_time_fm_epoch1_samples.png" alt="Time-conditioned samples after epoch 1">
            <figcaption><b>Epoch 1.</b> Early training usually yields noisy, partial shapes.</figcaption>
          </figure>
          <figure>
            <img src="images/p23_time_fm_epoch5_samples.png" alt="Time-conditioned samples after epoch 5">
            <figcaption><b>Epoch 5.</b> Shapes become more coherent as the vector field improves.</figcaption>
          </figure>
          <figure>
            <img src="images/p23_time_fm_epoch10_samples.png" alt="Time-conditioned samples after epoch 10">
            <figcaption><b>Epoch 10.</b> Digits sharpen and stabilize as time conditioning is learned.</figcaption>
          </figure>
        </div>
      </section>

      <!-- 2.4 -->
      <section class="subsection">
        <h2>2.4 Adding Class Conditioning (Digits 0–9)</h2>

        <p>
          Time conditioning tells the model “how far along the path we are.” Class conditioning tells the model “what digit we want.”
          We encode the class as a one-hot vector and inject it with two additional FCBlocks.
          To support classifier-free guidance, we randomly drop the class conditioning 10% of the time during training.
        </p>

        <pre class="code"><code># One-hot class vector c_onehot in R^10
c_onehot = torch.zeros(N, 10, device=x.device)
c_onehot.scatter_(1, c.view(-1,1), 1.0)

# Class dropout mask (p_uncond = 0.1)
keep = (torch.rand(N, device=x.device) > 0.1).float()
c_onehot = c_onehot * keep.view(N,1)

# Inject both c and t (affine-like modulation)
h_unflatten = c1 * h_unflatten + t1
h_up1       = c2 * h_up1       + t2</code></pre>

        <p class="walkthrough">
          <b>Walkthrough.</b>
          One-hot encoding means class “7” becomes a vector with a single 1 in the seventh position, and zeros elsewhere.
          The dropout mask sometimes zeroes out the class signal, forcing the UNet to learn an unconditional mode too.
          The modulation form <span class="mono">c·h + t</span> gives the model two levers: class can amplify/suppress feature channels,
          while time can shift features additively. This makes it easier to learn “digit identity” and “denoising stage” simultaneously.
        </p>
      </section>

      <!-- 2.5 -->
      <section class="subsection">
        <h2>2.5 Training the Class-conditioned UNet</h2>

        <figure class="wide">
          <img src="images/p25_class_fm_loss.png" alt="Class-conditioned flow matching training loss curve">
          <figcaption>
            <b>Deliverable: p25_class_fm_loss.png.</b> Training loss for class-conditioned flow matching.
          </figcaption>
        </figure>

        <p class="callout">
          <b>Practical note.</b> If your loss plateaus around ~1–1.3, the model may be collapsing toward “predict near-zero flow.”
          The most common causes are too-high learning rate, stepping the scheduler too frequently, or unstable conditioning injection.
        </p>
      </section>

      <!-- 2.6 -->
      <section class="subsection">
        <h2>2.6 Sampling with Classifier-free Guidance (CFG)</h2>

        <p>
          CFG combines unconditional and conditional predictions:
          it asks “what would you generate with no class?” and “what would you generate with class c?” then moves in the direction
          that emphasizes the class signal.
        </p>

        <pre class="code"><code># CFG sampling
x = torch.randn(N, 1, 28, 28, device=device)
dt = 1.0 / T

for i in range(T):
    t = torch.full((N,), i / T, device=device)

    u_uncond = u_theta(x, c, t, mask=zeros)  # class dropped
    u_cond   = u_theta(x, c, t, mask=ones)   # class provided

    u = u_uncond + gamma * (u_cond - u_uncond)
    x = x + dt * u</code></pre>

        <p class="walkthrough">
          <b>Walkthrough.</b>
          We compute two velocities. The unconditional one gives a “generic MNIST digit direction.”
          The conditional one gives a “specifically digit c direction.” The difference <span class="mono">(u_cond − u_uncond)</span>
          isolates what the class conditioning adds. Scaling it by γ increases class strength. If γ is too small,
          digits look random; if γ is too large, images can become distorted or overly confident. The update step
          applies this guided velocity repeatedly to transform noise into class-consistent digits.
        </p>

        <div class="grid cols-3">
          <figure>
            <img src="images/p26_class_fm_epoch1_cfg5_samples.png" alt="Class-conditioned CFG samples after epoch 1">
            <figcaption><b>Epoch 1 (CFG=5).</b> Often noisy and inconsistent early in training.</figcaption>
          </figure>
          <figure>
            <img src="images/p26_class_fm_epoch5_cfg5_samples.png" alt="Class-conditioned CFG samples after epoch 5">
            <figcaption><b>Epoch 5 (CFG=5).</b> Digits begin matching columns more reliably.</figcaption>
          </figure>
          <figure>
            <img src="images/p26_class_fm_epoch10_cfg5_samples.png" alt="Class-conditioned CFG samples after epoch 10">
            <figcaption><b>Epoch 10 (CFG=5).</b> Strong class separation: each column aims at a specific digit.</figcaption>
          </figure>
        </div>

        <section class="subsection">
          <h2>Reflection / Lessons Learned</h2>
          <p>
            The biggest conceptual shift was realizing that “denoising” doesn’t have to mean “predict the clean image.”
            Predicting the <i>motion</i> (flow) is often easier to learn because it turns generation into a controlled,
            step-by-step process. Conditioning then becomes natural: time says “how big of a correction should I apply now,”
            and class says “which attractor (digit identity) should my trajectory fall into.”
          </p>
        </section>

      </section>
    </section>

    <!-- ===================== IMPLEMENTATION NOTES ===================== -->
    <section id="impl-notes" class="section">
      <h1>Implementation Notes</h1>

      <div class="grid cols-2">
        <div class="card">
          <h3>Parameter choices</h3>
          <ul>
            <li><b>Single-step denoising</b>: D=128, σ=0.5, Adam lr=1e-4, batch=256, epochs=5</li>
            <li><b>Time-conditioned FM</b>: D=64, lr=1e-2 (often needs tuning), batch=64, epochs=10</li>
            <li><b>Class-conditioned FM</b>: D=64, p_uncond=0.1, T=300 steps, CFG γ=5.0</li>
          </ul>
          <p class="muted">
            If training is unstable, lower lr (e.g., 1e-3), add gradient clipping, and confirm scheduler stepping happens once per epoch.
          </p>
        </div>

        <div class="card">
          <h3>Common pitfalls (and how to avoid them)</h3>
          <ul>
            <li><b>Scheduler stepping per iteration</b>: your LR collapses too fast, model stops learning.</li>
            <li><b>Condition shapes</b>: ensure time is (N,1) before FCBlock, and reshape outputs to (N,C,1,1) for broadcasting.</li>
            <li><b>CFG formula</b>: use <span class="mono">u = u_uncond + γ (u_cond − u_uncond)</span> (difference, not sum).</li>
            <li><b>Clamping</b>: clamp only for visualization; clamping during training can hide error signals.</li>
          </ul>
        </div>
      </div>

      <p class="callout">
        <b>Debug trick.</b> Print the learning rate each epoch. If it shrinks every minibatch, your scheduler is in the wrong place.
        For older torch versions, use <span class="mono">StepLR(step_size=1)</span> as a drop-in replacement for ExponentLR.
      </p>
    </section>

    <!-- ===================== REFERENCES ===================== -->
    <section id="refs" class="section">
      <h1>References</h1>
      <ul>
        <li>PyTorch: <span class="mono">torch.nn.Conv2d</span>, <span class="mono">ConvTranspose2d</span>, <span class="mono">BatchNorm2d</span></li>
        <li>torchvision MNIST dataset</li>
        <li>UNet architecture idea: encoder-decoder with skip connections</li>
        <li>Flow Matching objective: supervised learning of a velocity field along interpolations</li>
      </ul>

      <p class="muted">
        Project website style inspired by CS180 Projects 3B / 4 / 5A conventions: dark academic theme, grid-based figures, and code + concept symmetry.
      </p>
    </section>

  </main>

  <footer class="footer">
    <div class="container footer-inner">
      <div>© 2025 Eduardo Cortes · CS180</div>
      <div class="muted">Website generated with AI assistance (ChatGPT), then edited and curated by the author.</div>
    </div>
  </footer>
</body>
</html>
