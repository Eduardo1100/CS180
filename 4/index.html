<!DOCTYPE html> 
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>CS180 Project 4 – Neural Radiance Fields (NeRF) https://eduardo1100.github.io/CS180/4/index.html#impl-notes</title>
  <meta name="description" content="CS180 Project 4 – Neural Radiance Fields: camera calibration, 2D neural fields, Lego NeRF, and NeRF on my own object." />

  <!-- Inter Font -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;800&display=swap" rel="stylesheet">

  <style>
    :root {
      --bg: #070b11;
      --bg-elevated: #0e141f;
      --bg-soft: #141b28;
      --ink: #e6edf7;
      --ink-muted: #9aa4c6;
      --accent: #6ab8ff;
      --accent-soft: rgba(106, 184, 255, 0.12);
      --border-subtle: #222b3c;
      --code-bg: #0b101a;
      --code-border: #222a3a;
      --callout-bg: #101827;
      --callout-border: #39567f;
      --danger: #ff6b7a;
      --success: #6bffb4;
      --max-width: 1100px;
      --radius-lg: 18px;
      --radius-sm: 10px;
      --shadow-soft: 0 22px 45px rgba(0, 0, 0, 0.55);
      --shadow-subtle: 0 16px 35px rgba(0, 0, 0, 0.35);
      --nav-height: 64px;
    }

    * { box-sizing: border-box; }
    html { scroll-behavior: smooth; }
    body {
      margin: 0;
      background: radial-gradient(circle at top, #181d2b 0, #05060a 55%, #020308 100%);
      color: var(--ink);
      font-family: "Inter", system-ui, -apple-system, BlinkMacSystemFont, sans-serif;
      line-height: 1.7;
      -webkit-font-smoothing: antialiased;
    }
    a { color: var(--accent); text-decoration: none; }
    a:hover { text-decoration: underline; }

    /* Top Navigation */
    header.top-nav {
      position: sticky; top: 0; z-index: 100; backdrop-filter: blur(18px);
      background: linear-gradient(to right, rgba(3,6,12,0.94), rgba(4,8,16,0.9));
      border-bottom: 1px solid rgba(255,255,255,0.04);
    }
    .nav-inner { max-width: var(--max-width); margin: 0 auto; padding: 12px 20px; display: flex; align-items: center; justify-content: space-between; gap: 16px; }
    .nav-title { font-size: 0.95rem; font-weight: 600; letter-spacing: 0.08em; text-transform: uppercase; color: var(--ink-muted); white-space: nowrap; }
    .nav-links { display: flex; flex-wrap: wrap; gap: 10px; font-size: 0.85rem; }
    .nav-links a { padding: 6px 10px; border-radius: 999px; border: 1px solid transparent; color: var(--ink-muted); background: transparent; transition: all 0.15s ease; }
    .nav-links a:hover { border-color: rgba(255,255,255,0.07); background: rgba(255,255,255,0.03); color: var(--ink); text-decoration: none; }
    .nav-links a.primary-link { border-color: rgba(255,255,255,0.08); background: radial-gradient(circle at top left, rgba(106,184,255,0.14), rgba(15,22,40,0.8)); color: var(--ink); }

    /* Layout */
    main { max-width: var(--max-width); margin: 0 auto; padding: 30px 18px 60px; }
    section { margin-bottom: 52px; padding: 26px 22px 28px; border-radius: var(--radius-lg); background: linear-gradient(145deg, rgba(11,16,27,0.98), rgba(5,7,13,0.98)); box-shadow: var(--shadow-soft); border: 1px solid rgba(255,255,255,0.02); }
    section.compact { margin-bottom: 30px; padding: 18px 18px 20px; box-shadow: var(--shadow-subtle); }
    h1, h2, h3, h4 { margin-top: 0; color: var(--ink); }
    h1 { font-size: 2.1rem; letter-spacing: 0.03em; text-transform: uppercase; font-weight: 800; margin-bottom: 0.5rem; }
    h2 { font-size: 1.4rem; margin-bottom: 0.4rem; border-bottom: 1px solid rgba(255,255,255,0.06); padding-bottom: 0.25rem; }
    h3 { font-size: 1.15rem; margin-top: 1.4rem; margin-bottom: 0.35rem; }
    h4 { font-size: 1.0rem; margin-top: 1.1rem; margin-bottom: 0.25rem; color: var(--ink-muted); text-transform: uppercase; letter-spacing: 0.08em; }
    p { margin-top: 0.4rem; margin-bottom: 0.6rem; color: var(--ink-muted); font-size: 0.96rem; }
    p.lead { font-size: 1.02rem; color: var(--ink); max-width: 80ch; }
    p.callout { margin-top: 0.8rem; margin-bottom: 0.8rem; padding: 12px 14px; border-radius: var(--radius-sm); background: var(--callout-bg); border: 1px solid var(--callout-border); color: var(--ink); font-size: 0.94rem; }
    p.callout span.label { font-size: 0.78rem; text-transform: uppercase; letter-spacing: 0.12em; color: var(--accent); display: inline-block; margin-bottom: 4px; }
    ul, ol { margin-top: 0.4rem; margin-bottom: 0.8rem; padding-left: 1.2rem; color: var(--ink-muted); font-size: 0.95rem; }
    li + li { margin-top: 0.18rem; }
    strong { color: var(--ink); }

    /* Grids & Figures */
    .grid { display: grid; gap: 14px; margin: 0.6rem 0 1.0rem; }
    .grid-2 { grid-template-columns: repeat(auto-fit, minmax(230px, 1fr)); }
    .grid-3 { grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); }
    .grid-4 { grid-template-columns: repeat(auto-fit, minmax(150px, 1fr)); }
    figure { margin: 0; padding: 10px; border-radius: 14px; background: var(--bg-soft); border: 1px solid rgba(255,255,255,0.05); box-shadow: 0 10px 25px rgba(0,0,0,0.35); }
    figure.wide { grid-column: 1 / -1; }
    figure img { display: block; width: 100%; border-radius: 10px; }
    figcaption { margin-top: 6px; font-size: 0.83rem; color: var(--ink-muted); }

    /* Code Blocks */
    pre.code { margin: 0.8rem 0 0.4rem; padding: 12px 14px; border-radius: var(--radius-sm); background: radial-gradient(circle at top left, #121b30, #050811); border: 1px solid var(--code-border); overflow-x: auto; box-shadow: 0 12px 28px rgba(0,0,0,0.55); }
    pre.code code { font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, "Cascadia Code", "Fira Code", monospace; font-size: 0.86rem; color: #f3f4ff; }
    .code .kw { color: #9cdcfe; }
    .code .fn { color: #dcdcaa; }
    .code .str { color: #ce9178; }
    .code .num { color: #b5cea8; }
    .code .cm { color: #6a9955; }

    .walkthrough { margin: 0.3rem 0 0.9rem; padding: 10px 12px 6px; border-left: 2px solid rgba(255,255,255,0.12); background: linear-gradient(to right, rgba(255,255,255,0.02), transparent); border-radius: 0 10px 10px 0; }
    .walkthrough p { font-size: 0.93rem; }
    .walkthrough ol { font-size: 0.9rem; }

    .meta-row { display: flex; flex-wrap: wrap; align-items: center; gap: 8px; margin-bottom: 0.4rem; font-size: 0.8rem; color: var(--ink-muted); text-transform: uppercase; letter-spacing: 0.14em; }
    .meta-pill { padding: 4px 8px; border-radius: 999px; border: 1px solid rgba(255,255,255,0.08); background: rgba(255,255,255,0.02); }

    footer { max-width: var(--max-width); margin: 0 auto 32px; padding: 14px 18px 6px; border-radius: var(--radius-lg); border: 1px solid rgba(255,255,255,0.04); background: radial-gradient(circle at top left, rgba(255,255,255,0.03), rgba(1,2,5,0.98)); font-size: 0.8rem; color: var(--ink-muted); }
    footer p { margin: 4px 0; }
    @media (max-width: 720px) { .nav-inner { padding-inline: 14px; } main { padding-inline: 14px; } section { padding-inline: 16px; } h1 { font-size: 1.7rem; } .nav-title { display: none; } }
  </style>
</head>

<body>
  <!-- Sticky Navigation -->
  <header class="top-nav">
    <div class="nav-inner">
      <div class="nav-title">CS180 · Project 4 · Neural Radiance Fields</div>
      <nav class="nav-links">
        <a href="#overview" class="primary-link">Overview</a>
        <a href="#part0">Part 0 – Calibration & Scan</a>
        <a href="#part1">Part 1 – 2D Neural Field</a>
        <a href="#part2">Part 2 – Lego NeRF</a>
        <a href="#part26">Part 2.6 – My Object NeRF</a>
        <a href="#impl-notes">Implementation Notes</a>
        <a href="#references">References</a>
      </nav>
    </div>
  </header>

  <main>
    <!-- Overview -->
    <section id="overview">
      <div class="meta-row">
        <span class="meta-pill">CS180 · Intro to Computer Vision and Computational Photography</span>
        <span class="meta-pill">Fall 2025</span>
        <span class="meta-pill">Neural Radiance Fields</span>
      </div>
      <h1>Project 4 – Neural Radiance Fields (NeRF)</h1>
      <p class="lead">
        In this project I go from raw phone images of a small object to a fully learned, continuous 3D representation using
        <strong>Neural Radiance Fields (NeRF)</strong>. The pipeline includes camera calibration and pose estimation, fitting a
        2D neural field on an image, training a full NeRF on the classic Lego dataset, and finally learning a NeRF of my own object.
      </p>
      <p class="callout">
        <span class="label">High-level Intuition</span><br />
        Rather than storing a 3D model as meshes or voxels, NeRF learns a <em>continuous function</em> that, given a point in 3D space
        and a viewing direction, predicts color and density. By integrating these predictions along camera rays, we can render
        realistic images from new viewpoints.
      </p>
    </section>

    <!-- Part 0: Camera Calibration & 3D Capture -->
    <section id="part0">
      <h2>Part 0 – Calibrating Your Camera and Capturing a 3D Scan (0.1–0.4)</h2>
      <p>
        Before training any NeRF, we need accurate camera parameters. This part covers:
      </p>
      <ul>
        <li><strong>0.1 – Camera calibration</strong> with ArUco tags to recover intrinsics.</li>
        <li><strong>0.2 – Object capture</strong> with consistent lighting and distance.</li>
        <li><strong>0.3 – Pose estimation</strong> using Perspective-n-Point (PnP).</li>
        <li><strong>0.4 – Undistortion and dataset packaging</strong> into the NeRF-ready <code>.npz</code> format.</li>
      </ul>

      <h3>0.1 Camera Calibration with ArUco Tags</h3>
      <p>
        I printed an ArUco tag grid and captured 30–50 images from different angles while keeping the phone’s focal length fixed.
        OpenCV detects the corners in 2D, which I associate with known 3D points on the flat tag. From those correspondences,
        <code>cv2.calibrateCamera</code> estimates the camera intrinsics matrix and lens distortion.
      </p>

      <pre class="code"><code>
<span class="kw">import</span> cv2, numpy <span class="kw">as</span> np

aruco_dict = cv2.aruco.getPredefinedDictionary(cv2.aruco.DICT_4X4_50)
aruco_params = cv2.aruco.DetectorParameters()

objpoints, imgpoints = [], []

<span class="kw">def</span> <span class="fn">tag_corners_3d</span>(tag_size_m=0.02):
    s = tag_size_m
    <span class="kw">return</span> np.array([
        [0.0, 0.0, 0.0],
        [s,   0.0, 0.0],
        [s,   s,   0.0],
        [0.0, s,   0.0],
    ], dtype=np.float32)

world_corners = tag_corners_3d()

<span class="kw">for</span> path <span class="kw">in</span> calibration_image_paths:
    img = cv2.imread(path)
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

    corners, ids, _ = cv2.aruco.detectMarkers(gray, aruco_dict, parameters=aruco_params)
    <span class="kw">if</span> ids <span class="kw">is</span> <span class="kw">None</span>:
        <span class="kw">continue</span>

    img_corners = np.concatenate(corners, axis=0).reshape(-1, 2).astype(np.float32)
    imgpoints.append(img_corners)

    obj_corners = np.tile(world_corners, (len(ids), 1))
    objpoints.append(obj_corners)

ret, K, dist_coeffs, rvecs, tvecs = cv2.calibrateCamera(
    objpoints, imgpoints, gray.shape[::-1], <span class="kw">None</span>, <span class="kw">None</span>
)
      </code></pre>

      <div class="walkthrough">
        <h4>Walkthrough</h4>
        <p>
          The goal here is to translate the visual pattern on the ArUco grid into numerical constraints the solver can use to
          infer the camera’s focal length and principal point.
        </p>
        <ol>
          <li><strong>ArUco setup:</strong> I create the predefined 4×4 ArUco dictionary and detector parameters. This tells OpenCV which family of markers to look for.</li>
          <li><strong>3D tag geometry:</strong> <code>tag_corners_3d</code> returns the four corners of a square tag in meters. I treat the tag as lying on the <code>z = 0</code> plane.</li>
          <li><strong>Loop over images:</strong> For each calibration photo, I convert to grayscale and run <code>detectMarkers</code>. If detection fails, I skip the image so the pipeline is robust.</li>
          <li><strong>Collect 2D points:</strong> I concatenate all detected corners into a single 2D array <code>img_corners</code>, which holds pixel coordinates.</li>
          <li><strong>Collect 3D points:</strong> For every detected tag, I append a copy of the 3D tag corner coordinates to <code>objpoints</code>, giving the solver many 3D–2D correspondences.</li>
          <li><strong>Calibration:</strong> Finally, <code>cv2.calibrateCamera</code> estimates <code>K</code> (intrinsics) and <code>dist_coeffs</code> (lens distortion). These are the foundations for all later pose and NeRF computations.</li>
        </ol>
      </div>

      <h3>0.2 Object Capture</h3>
      <p>
        I chose a small object, placed a single ArUco tag on the table next to it, and captured 30–50 photos while moving the camera
        in an arc around the object. I tried to keep:
      </p>
      <ul>
        <li><strong>Exposure</strong> roughly constant (no automatic brightness jumps).</li>
        <li><strong>Blur</strong> minimal by holding the phone steady.</li>
      </ul>

      <h3>0.3 Pose Estimation and Viser Visualization</h3>
      <p>
        Using the intrinsics and distortion coefficients from calibration, I estimate the camera pose for each object image with
        <code>cv2.solvePnP</code>. This gives me the camera’s rotation and translation relative to the tag, which I convert into a
        camera-to-world matrix (<code>c2w</code>). I then visualize all the camera frustums in 3D using <code>viser</code>.
      </p>

      <div class="grid grid-2">
        <figure class="wide">
          <img src="images/part0_camera_calibration/viser_frustum_view1.png" alt="Viser visualization of camera frustums – view 1" />
          <figcaption>Viser visualization of all camera frustums orbiting the object (view 1).</figcaption>
        </figure>
        <figure class="wide">
          <img src="images/part0_camera_calibration/viser_frustum_view2.png" alt="Viser visualization of camera frustums – view 2" />
          <figcaption>Another viewpoint of the same cloud of cameras, confirming a reasonably smooth arc.</figcaption>
        </figure>
      </div>

      <h3>0.4 Undistortion & Dataset Packaging</h3>
      <p>
        NeRF assumes a simple pinhole camera model without lens distortion, so I undistort every image and crop valid pixels using
        <code>cv2.getOptimalNewCameraMatrix</code>. I then build a <code>.npz</code> containing images and their corresponding
        <code>c2w</code> matrices, split into train/val/test.
      </p>
    </section>

    <!-- Part 1: 2D Neural Field -->
    <section id="part1">
      <h2>Part 1 – Fit a Neural Field to a 2D Image (1.1–1.4)</h2>
      <p>
        Before tackling full 3D NeRFs, I first train a <strong>2D neural field</strong> that maps pixel coordinates to colors in a single
        image. This is an easier sandbox to understand positional encoding, MLP architecture, and training behavior.
      </p>

      <h3>1.1 Objective & Intuition</h3>
      <p>
        The neural field is a function <code>F(u, v) → RGB</code> that takes continuous, normalized pixel coordinates and outputs
        the color at that point. Instead of storing the image as a grid of values, I store it as the weights of a neural network—
        a kind of compressed, continuous representation.
      </p>

      <h3>1.2 Network & Positional Encoding</h3>
      <p>
        I use a small MLP with sinusoidal positional encoding (PE). PE expands coordinates into a higher-dimensional space using
        sines and cosines, enabling the network to capture fine details and edges.
      </p>

      <pre class="code"><code>
<span class="kw">import</span> torch
<span class="kw">import</span> torch.nn <span class="kw">as</span> nn
<span class="kw">import</span> torch.nn.functional <span class="kw">as</span> F

<span class="kw">class</span> <span class="fn">PosEnc</span>(nn.Module):
    <span class="kw">def</span> <span class="fn">__init__</span>(self, num_freqs: <span class="kw">int</span> = 10):
        <span class="kw">super</span>().__init__()
        self.num_freqs = num_freqs

    <span class="kw">def</span> <span class="fn">forward</span>(self, x):
        encodings = [x]
        <span class="kw">for</span> i <span class="kw">in</span> <span class="fn">range</span>(self.num_freqs):
            freq = 2.0 ** i * torch.pi
            encodings.append(torch.sin(freq * x))
            encodings.append(torch.cos(freq * x))
        <span class="kw">return</span> torch.cat(encodings, dim=-1)


<span class="kw">class</span> <span class="fn">NeuralField2D</span>(nn.Module):
    <span class="kw">def</span> <span class="fn">__init__</span>(self, width=128, num_freqs=10):
        <span class="kw">super</span>().__init__()
        self.pe = PosEnc(num_freqs)
        in_dim = 2 + 2 * 2 * num_freqs

        layers = []
        hidden_dims = [width] * 4
        last_dim = in_dim
        <span class="kw">for</span> h <span class="kw">in</span> hidden_dims:
            layers.append(nn.Linear(last_dim, h))
            layers.append(nn.ReLU(inplace=<span class="kw">True</span>))
            last_dim = h

        self.mlp = nn.Sequential(*layers)
        self.out_layer = nn.Sequential(
            nn.Linear(last_dim, 3),
            nn.Sigmoid(),
        )

    <span class="kw">def</span> <span class="fn">forward</span>(self, uv):
        x = self.pe(uv)
        h = self.mlp(x)
        rgb = self.out_layer(h)
        <span class="kw">return</span> rgb
      </code></pre>

      <div class="walkthrough">
        <h4>Walkthrough</h4>
        <p>
          This block defines the core 2D neural field model. It hides most of the math of “fitting a function to an image”
          inside a simple PyTorch module.
        </p>
        <ol>
          <li><strong><code>PosEnc</code>:</strong> The positional encoding layer takes raw <code>(u, v)</code> coordinates in <code>[0, 1]</code> and builds a richer feature vector using sine and cosine at exponentially increasing frequencies. This lets the MLP represent both smooth regions and sharp edges.</li>
          <li><strong>Frequency loop:</strong> For each <code>i</code>, I compute <code>freq = 2^i · π</code>. Applying <code>sin(freq · x)</code> and <code>cos(freq · x)</code> at multiple frequencies effectively creates a Fourier-like basis over coordinates.</li>
          <li><strong>Input dimension:</strong> The MLP input has the original 2 coordinates plus <code>2*(sin, cos)*num_freqs</code> values per coordinate. Concatenating all of these yields a high-dimensional input describing the position.</li>
          <li><strong>MLP architecture:</strong> I use 4 fully connected layers with ReLU activations. This is enough capacity to memorize a moderate-resolution image without being too slow.</li>
          <li><strong>Output layer:</strong> The final <code>Linear → Sigmoid</code> block maps to three channels in <code>[0, 1]</code>, which correspond directly to RGB values.</li>
        </ol>
      </div>

      <h3>1.3 Training & PSNR</h3>
      <p>
        During training, I randomly sample 10k pixels at each iteration, feed their normalized coordinates into the network,
        and compare predicted colors against ground truth using mean squared error (MSE). I track reconstruction quality using
        PSNR (Peak Signal-to-Noise Ratio).
      </p>

      <pre class="code"><code>
<span class="kw">def</span> <span class="fn">train_2d_field</span>(
    target_img,
    num_iters=2000,
    batch_size=8192,
    width=128,
    num_freqs=10,
    device=<span class="kw">None</span>,
):
    <span class="kw">if</span> device <span class="kw">is</span> <span class="kw">None</span>:
        device = torch.device(<span class="str">"cuda"</span> <span class="kw">if</span> torch.cuda.is_available() <span class="kw">else</span> <span class="str">"cpu"</span>)

    target = torch.as_tensor(target_img, dtype=torch.float32, device=device)
    target = target / 255.0 <span class="kw">if</span> target.max() &gt; 1.0 <span class="kw">else</span> target
    H, W, _ = target.shape

    xs = torch.linspace(0.0, 1.0, W, device=device)
    ys = torch.linspace(0.0, 1.0, H, device=device)
    grid_x, grid_y = torch.meshgrid(xs, ys, indexing=<span class="str">"xy"</span>)
    coords = torch.stack([grid_x, grid_y], dim=-1).view(-1, 2)
    colors = target.view(-1, 3)

    model = NeuralField2D(width=width, num_freqs=num_freqs).to(device)
    opt = torch.optim.Adam(model.parameters(), lr=1e-3)

    psnr_history = []

    <span class="kw">for</span> it <span class="kw">in</span> <span class="fn">range</span>(num_iters):
        idx = torch.randint(0, coords.shape[0], (batch_size,), device=device)
        uv = coords[idx]
        rgb_gt = colors[idx]

        rgb_pred = model(uv)
        loss = F.mse_loss(rgb_pred, rgb_gt)

        opt.zero_grad()
        loss.backward()
        opt.step()

        mse = loss.detach()
        psnr = -10.0 * torch.log10(mse)
        psnr_history.append(psnr.item())

    <span class="kw">return</span> model, psnr_history
      </code></pre>

      <div class="walkthrough">
        <h4>Walkthrough</h4>
        <p>
          This training loop turns the static image into a dataset of coordinates and colors, then optimizes the MLP to regress
          from the former to the latter.
        </p>
        <ol>
          <li><strong>Coordinate grid:</strong> I create a full grid of normalized <code>(u, v)</code> coordinates and flatten it so each row corresponds to a pixel.</li>
          <li><strong>Targets:</strong> The target image is reshaped to match, so each coordinate has a matching RGB color.</li>
          <li><strong>Mini-batches:</strong> At every iteration, I sample <code>batch_size</code> random pixels to improve convergence speed and generalization.</li>
          <li><strong>Optimization:</strong> I use Adam with a small learning rate and optimize MSE between predicted and target colors.</li>
          <li><strong>PSNR tracking:</strong> I convert the MSE on each batch into PSNR and store it for plotting the learning curve.</li>
        </ol>
      </div>

      <div class="grid grid-3">
        <figure>
          <img src="images/part1_2d_neural_field/train_provided_iter0000.png" alt="Training progression on provided image, iteration 0" />
          <figcaption>Provided image: random initialization (iteration 0).</figcaption>
        </figure>
        <figure>
          <img src="images/part1_2d_neural_field/train_provided_iter0300.png" alt="Training progression on provided image, iteration 300" />
          <figcaption>Provided image: structure emerging (iteration 300).</figcaption>
        </figure>
        <figure>
          <img src="images/part1_2d_neural_field/train_provided_iter2000.png" alt="Training progression on provided image, iteration 2000" />
          <figcaption>Provided image: final reconstruction closely matches the original (iteration 2000).</figcaption>
        </figure>
      </div>

      <div class="grid grid-3">
        <figure>
          <img src="images/part1_2d_neural_field/train_own_iter0000.png" alt="Training progression on my own image, iteration 0" />
          <figcaption>My own image: random initialization (iteration 0).</figcaption>
        </figure>
        <figure>
          <img src="images/part1_2d_neural_field/train_own_iter0300.png" alt="Training progression on my own image, iteration 300" />
          <figcaption>My own image: major structure appears within a few hundred steps (iteration 300).</figcaption>
        </figure>
        <figure>
          <img src="images/part1_2d_neural_field/train_own_iter2000.png" alt="Training progression on my own image, iteration 2000" />
          <figcaption>My own image: final neural reconstruction (iteration 2000).</figcaption>
        </figure>
      </div>

      <figure class="wide">
        <img src="images/part1_2d_neural_field/psnr_curve_own.png" alt="PSNR curve over training iterations for the 2D neural field" />
        <figcaption>PSNR vs iteration for the 2D neural field. The curve plateaus when the network has fully memorized the image.</figcaption>
      </figure>

      <h3>1.4 Hyperparameter Sweeps</h3>
      <p>
        I sweep over two key hyperparameters: the width of the MLP and the maximum PE frequency <code>L</code>. The following grid
        shows how capacity and frequency content affect sharpness:
      </p>

      <div class="grid grid-2">
        <figure>
          <img src="images/part1_2d_neural_field/train_grid_L4_W64_iter1000.png" alt="Low frequency, low width" />
          <figcaption>Low PE frequency = 4, low width = 64: overly smooth, blurred edges.</figcaption>
        </figure>
        <figure>
          <img src="images/part1_2d_neural_field/train_grid_L4_W128_iter1000.png" alt="Low frequency, high width" />
          <figcaption>Low PE frequency = 4, high width = 128: more capacity but still lacking fine detail.</figcaption>
        </figure>
        <figure>
          <img src="images/part1_2d_neural_field/train_grid_L10_W64_iter1000.png" alt="High frequency, low width" />
          <figcaption>High PE frequency = 10, low width = 64: some high-frequency detail, but underfit in complex regions.</figcaption>
        </figure>
        <figure>
          <img src="images/part1_2d_neural_field/train_grid_L10_W128_iter1000.png" alt="High frequency, high width" />
          <figcaption>High PE frequency = 10, high width = 128: sharpest reconstruction, but with higher computational cost.</figcaption>
        </figure>
      </div>

      <p class="callout">
        <span class="label">Takeaway</span><br />
        Positional encoding is what allows the MLP to represent crisp edges and small details, while width controls how much capacity
        the network has to memorize complex textures. Together, they determine the trade-off between smoothness and fidelity.
      </p>
    </section>

    <!-- Part 2: Lego NeRF -->
    <section id="part2">
      <h2>Part 2 – Fit a Neural Radiance Field from Multi-view Images (2.1–2.5)</h2>
      <p>
        With 2D neural fields working, I move to the full 3D NeRF setup on the classic Lego dataset. Here the model takes 3D points
        and viewing directions as input and predicts both density and color, which are combined using volume rendering.
      </p>

      <h3>2.1 Rays from Cameras</h3>
      <p>
        I first convert pixel coordinates into 3D rays using the camera intrinsics and camera-to-world matrices. Each ray has an
        origin (camera center) and a direction in world space.
      </p>

      <pre class="code"><code>
<span class="kw">import</span> torch

<span class="kw">def</span> <span class="fn">pixel_to_camera</span>(K, uv, depth=1.0):
    fx, fy = K[0, 0], K[1, 1]
    cx, cy = K[0, 2], K[1, 2]

    u, v = uv[..., 0], uv[..., 1]
    x = (u - cx) / fx * depth
    y = (v - cy) / fy * depth
    z = torch.full_like(x, depth)
    <span class="kw">return</span> torch.stack([x, y, z], dim=-1)


<span class="kw">def</span> <span class="fn">pixel_to_ray</span>(K, c2w, uv):
    depth = 1.0
    cam_pts = pixel_to_camera(K, uv, depth)

    R = c2w[:3, :3]
    t = c2w[:3, 3]
    world_pts = (R @ cam_pts.T).T + t

    ray_o = t.expand_as(world_pts)
    ray_d = world_pts - ray_o
    ray_d = ray_d / ray_d.norm(dim=-1, keepdim=<span class="kw">True</span>)
    <span class="kw">return</span> ray_o, ray_d
      </code></pre>

      <div class="walkthrough">
        <h4>Walkthrough</h4>
        <p>
          This code bridges 2D image space and 3D world space, which is essential for NeRF: we must know which 3D line each pixel corresponds to.
        </p>
        <ol>
          <li><strong><code>pixel_to_camera</code>:</strong> Uses the pinhole camera model: subtract the principal point <code>(cx, cy)</code>, divide by focal lengths, and scale by depth. This “unprojects” a pixel into a 3D point at distance 1 along the camera’s optical axis.</li>
          <li><strong>Camera coordinates to world coordinates:</strong> Multiplying by <code>R</code> and adding <code>t</code> effectively rotates and translates points from the camera’s frame to the world frame.</li>
          <li><strong>Ray origin:</strong> The ray origin is just the camera center, which is <code>t</code> when <code>c2w</code> is camera-to-world.</li>
          <li><strong>Ray direction:</strong> I compute <code>world_pts - ray_o</code> and normalize it to get a unit direction vector.</li>
          <li><strong>Batching:</strong> Everything is implemented in a vectorized way to efficiently handle many rays per training step.</li>
        </ol>
      </div>

      <figure class="wide">
        <img src="images/part2_lego_nerf/viser_rays_and_samples_1.png" alt="Visualization of cameras, rays, and samples in Viser" />
        <figcaption>Viser visualization showing camera frustums, sampled rays, and 3D points along them.</figcaption>
      </figure>
      <figure class="wide">
        <img src="images/part2_lego_nerf/viser_rays_and_samples_2.png" alt="Visualization of cameras, rays, and samples in Viser" />
        <figcaption>Viser visualization showing camera frustums, sampled rays, and 3D points along them.</figcaption>
      </figure>

      <div class="walkthrough">
        <h4>Viser validation visuals?</h4>
        <p>
          These 3D plots are a quick, visual “sanity check” that all upstream geometry is consistent <em>before</em> training NeRF. They confirm that
          intrinsics (from calibration), undistortion, PnP poses, and my camera-to-world (<code>c2w</code>) convention agree with each other in a single,
          real-world coordinate frame.
        </p>
        <ul>
          <li><strong>Pose coherence:</strong> Camera frustums form a smooth arc around the object, with consistent “up” direction. Sudden jumps or twists usually mean a flipped axis or mixed conventions (<code>w2c</code> vs <code>c2w</code>).</li>
          <li><strong>Scale & bounds:</strong> Distances between cameras and the tag/object look physically plausible, helping me pick reasonable <code>near</code>/<code>far</code> ranges for ray sampling.</li>
          <li><strong>Coverage:</strong> The orbit shows whether viewpoints sufficiently wrap the object (front, sides, some elevation). Sparse or clustered views predict holes/blur in the NeRF.</li>
          <li><strong>Error spotting:</strong> Mis-undistortion, wrong principal point, or transposed rotations manifest immediately as crossed frustums, shears, or cameras pointing the wrong way.</li>
        </ul>
        <p class="callout">
          <span class="label">How to read the plots</span><br/>
          The pyramids are camera “cones” pointing where each photo looked. A clean ring with cones aimed at the same target implies consistent
          poses. If cones diverge, flip, or intersect oddly, fix calibration/EXIF/pose code <em>before</em> training—otherwise the NeRF will try to
          explain bad geometry with blurry density.
        </p>
      </div>

      <h3>2.2 Sampling Points along Rays</h3>
      <p>
        For each ray, I sample a set of points between a near and far bound (2.0 and 6.0 for the Lego scene). During training,
        I add small random perturbations to encourage the model to cover the entire interval and avoid overfitting to a fixed grid.
      </p>

      <pre class="code"><code>
<span class="kw">def</span> <span class="fn">sample_along_rays</span>(rays_o, rays_d, n_samples=64, near=2.0, far=6.0, perturb=<span class="kw">True</span>):
    B = rays_o.shape[0]
    t_vals = torch.linspace(near, far, n_samples, device=rays_o.device)
    t_vals = t_vals.expand(B, n_samples)

    <span class="kw">if</span> perturb:
        mids = 0.5 * (t_vals[:, :-1] + t_vals[:, 1:])
        widths = t_vals[:, 1:] - t_vals[:, :-1]
        noise = (torch.rand_like(mids) - 0.5) * widths
        t_vals = torch.cat([mids + noise, t_vals[:, -1:]], dim=-1)

    points = rays_o[..., <span class="num">None</span>, :] + rays_d[..., <span class="num">None</span>, :] * t_vals[..., <span class="num">None</span>]
    <span class="kw">return</span> points, t_vals
      </code></pre>

      <div class="walkthrough">
        <h4>Walkthrough</h4>
        <p>
          NeRF is essentially integrating along each ray, so we approximate that integral by sampling discrete points.
        </p>
        <ol>
          <li><strong>Base sampling:</strong> <code>torch.linspace(near, far, n_samples)</code> gives evenly spaced depths along the ray. Each ray shares the same initial sample depths.</li>
          <li><strong>Perturbation:</strong> To avoid aliasing artifacts, I jitter the sample positions inside each interval. This is similar to anti-aliasing in rendering and helps cover the continuous volume more uniformly.</li>
          <li><strong>3D point computation:</strong> The formula <code>rays_o + rays_d * t</code> gives the 3D point at distance <code>t</code> along the ray.</li>
          <li><strong>Shape:</strong> The result <code>points</code> has shape <code>[B, N, 3]</code>, which is convenient for feeding into the NeRF MLP.</li>
        </ol>
      </div>

      <h3>2.3 NeRF Network Architecture</h3>
      <p>
        The NeRF network takes in 3D points and viewing directions, applies separate positional encodings, and outputs a density
        (scalar) and an RGB color conditioned on direction.
      </p>

      <pre class="code"><code>
<span class="kw">class</span> <span class="fn">NeRF</span>(nn.Module):
    <span class="kw">def</span> <span class="fn">__init__</span>(self, pos_freqs=10, dir_freqs=4, width=256):
        <span class="kw">super</span>().__init__()
        self.pos_pe = PosEnc(pos_freqs)
        self.dir_pe = PosEnc(dir_freqs)

        pos_dim = 3 + 2 * 3 * pos_freqs
        dir_dim = 3 + 2 * 3 * dir_freqs

        self.fc_pos = nn.Sequential(
            nn.Linear(pos_dim, width), nn.ReLU(<span class="kw">True</span>),
            nn.Linear(width, width),   nn.ReLU(<span class="kw">True</span>),
            nn.Linear(width, width),   nn.ReLU(<span class="kw">True</span>),
            nn.Linear(width, width),   nn.ReLU(<span class="kw">True</span>),
        )

        self.fc_pos2 = nn.Sequential(
            nn.Linear(width + pos_dim, width),
            nn.ReLU(<span class="kw">True</span>),
        )

        self.sigma_head = nn.Sequential(
            nn.Linear(width, 1),
            nn.ReLU(),
        )

        self.fc_feat = nn.Linear(width, width)
        self.fc_rgb = nn.Sequential(
            nn.Linear(width + dir_dim, width // 2),
            nn.ReLU(<span class="kw">True</span>),
            nn.Linear(width // 2, 3),
            nn.Sigmoid(),
        )

    <span class="kw">def</span> <span class="fn">forward</span>(self, x, d):
        B, N, _ = x.shape

        x_enc = self.pos_pe(x.view(-1, 3))
        h = self.fc_pos(x_enc)
        h = self.fc_pos2(torch.cat([h, x_enc], dim=-1))

        sigma = self.sigma_head(h)

        d_enc = self.dir_pe(d.view(-1, 3))
        feat = self.fc_feat(h)
        h_color = torch.cat([feat, d_enc], dim=-1)
        rgb = self.fc_rgb(h_color)

        sigma = sigma.view(B, N, 1)
        rgb = rgb.view(B, N, 3)
        <span class="kw">return</span> sigma, rgb
      </code></pre>

      <div class="walkthrough">
        <h4>Walkthrough</h4>
        <p>
          This is the core of the NeRF: a network that translates coordinates and directions into physical quantities used by the volume renderer.
        </p>
        <ol>
          <li><strong>Separate encodings:</strong> I encode <code>x</code> (3D position) and <code>d</code> (view direction) separately. Positions usually require higher frequencies than directions.</li>
          <li><strong>Position branch:</strong> The encoded position goes through several fully connected layers with ReLU, forming a deep feature representation of local geometry.</li>
          <li><strong>Skip connection:</strong> Concatenating <code>h</code> with <code>x_enc</code> mid-way helps the network keep track of the original spatial location and improves training stability.</li>
          <li><strong>Density head:</strong> The <code>sigma_head</code> predicts a non-negative density through a ReLU, representing how much light is absorbed or scattered at each point.</li>
          <li><strong>Color head:</strong> The color branch takes both the feature vector from geometry and the encoded viewing direction. This allows the network to model view-dependent effects like specular highlights.</li>
          <li><strong>Reshaping:</strong> After computing <code>sigma</code> and <code>rgb</code> for <code>B·N</code> points, I reshape them back to <code>[B, N, ·]</code> so they line up with the sampled points along each ray.</li>
        </ol>
      </div>

      <h3>2.4 Volume Rendering</h3>
      <p>
        To render a pixel, I convert densities to opacities and integrate colors along the ray using the NeRF volume rendering equation.
        In discrete form, each sample contributes:
      </p>
      <p>
        <code>color = Σ T<sub>i</sub> · α<sub>i</sub> · c<sub>i</sub></code>, where <code>T<sub>i</sub></code> is the accumulated transmittance up to sample i,
        and <code>α<sub>i</sub> = 1 - exp(-σ<sub>i</sub> Δt)</code> is the opacity.
      </p>

      <pre class="code"><code>
<span class="kw">def</span> <span class="fn">volume_render</span>(sigmas, rgbs, t_vals):
    B, N, _ = sigmas.shape
    deltas = t_vals[:, 1:] - t_vals[:, :-1]
    deltas = torch.cat([deltas, deltas[:, -1:]], dim=-1)

    alpha = 1.0 - torch.exp(-sigmas.squeeze(-1) * deltas)

    accum = torch.cumsum(-sigmas.squeeze(-1) * deltas, dim=-1)
    T = torch.exp(torch.cat([torch.zeros(B, 1, device=sigmas.device), accum[:, :-1]], dim=-1))

    weights = T * alpha
    rgb_map = (weights[..., <span class="num">None</span>] * rgbs).sum(dim=1)
    <span class="kw">return</span> rgb_map
      </code></pre>

      <div class="walkthrough">
        <h4>Walkthrough</h4>
        <p>
          This function numerically approximates the continuous volume rendering integral using the discrete samples produced earlier.
        </p>
        <ol>
          <li><strong>Δt computation:</strong> <code>deltas</code> stores the distance between consecutive samples along the ray. The last interval is copied from the previous one.</li>
          <li><strong>Opacity α:</strong> I convert densities into opacities using <code>α = 1 − exp(−σ Δt)</code>. This comes from the Beer–Lambert law of light attenuation.</li>
          <li><strong>Transmittance T:</strong> The cumulative sum of <code>−σ Δt</code> gives the accumulated optical thickness. Exponentiating yields transmittance: the probability that light survives from the ray origin to the current sample.</li>
          <li><strong>Weights:</strong> Each sample’s contribution is <code>T · α</code>, meaning “the ray reaches this point and then terminates here.”</li>
          <li><strong>Final color:</strong> I multiply each sample’s color by its weight and sum along the ray dimension, producing one RGB value per ray.</li>
        </ol>
      </div>

      <h3>2.5 Training & Results on the Lego Scene</h3>
      <p>
        I train NeRF on the Lego dataset using Adam (learning rate 5e-4) with 10k rays per iteration. The validation PSNR reaches
        above the 23 dB target within 1000 gradient steps.
      </p>

      <pre class="code"><code>
<span class="kw">def</span> <span class="fn">train_nerf_lego</span>(
    images,
    c2ws,
    focal,
    num_iters=1000,
    rays_per_batch=10000,
    n_samples=64,
    near=2.0,
    far=6.0,
    device=<span class="kw">None</span>,
):
    <span class="kw">if</span> device <span class="kw">is</span> <span class="kw">None</span>:
        device = torch.device(<span class="str">"cuda"</span> <span class="kw">if</span> torch.cuda.is_available() <span class="kw">else</span> <span class="str">"cpu"</span>)

    imgs = torch.as_tensor(images, dtype=torch.float32, device=device)
    imgs = imgs / 255.0 <span class="kw">if</span> imgs.max() &gt; 1.0 <span class="kw">else</span> imgs
    N, H, W, _ = imgs.shape
    imgs_flat = imgs.view(-1, 3)

    K = torch.tensor(
        [[focal, 0.0, W / 2.0],
         [0.0,   focal, H / 2.0],
         [0.0,   0.0,   1.0]],
        dtype=torch.float32,
        device=device,
    )

    ix, iy = torch.meshgrid(
        torch.arange(W, device=device),
        torch.arange(H, device=device),
        indexing=<span class="str">"xy"</span>
    )
    uv = torch.stack([ix + 0.5, iy + 0.5], dim=-1).view(-1, 2)

    rays_o_all = []
    rays_d_all = []
    <span class="kw">for</span> c2w <span class="kw">in</span> c2ws:
        c2w_t = torch.as_tensor(c2w, dtype=torch.float32, device=device)
        ray_o, ray_d = pixel_to_ray(K, c2w_t, uv)
        rays_o_all.append(ray_o)
        rays_d_all.append(ray_d)

    rays_o_all = torch.cat(rays_o_all, dim=0)
    rays_d_all = torch.cat(rays_d_all, dim=0)

    model = NeRF().to(device)
    opt = torch.optim.Adam(model.parameters(), lr=5e-4)

    psnr_history = []

    <span class="kw">for</span> it <span class="kw">in</span> <span class="fn">range</span>(num_iters):
        idx = torch.randint(0, rays_o_all.shape[0], (rays_per_batch,), device=device)
        rays_o = rays_o_all[idx]
        rays_d = rays_d_all[idx]
        target = imgs_flat[idx]

        pts, t_vals = sample_along_rays(rays_o, rays_d, n_samples, near, far, perturb=<span class="kw">True</span>)
        sigmas, rgbs = model(
            pts,
            rays_d[..., <span class="num">None</span>, :].expand_as(pts),
        )
        rgb_map = volume_render(sigmas, rgbs, t_vals)

        loss = F.mse_loss(rgb_map, target)

        opt.zero_grad()
        loss.backward()
        opt.step()

        mse = loss.detach()
        psnr = -10.0 * torch.log10(mse)
        psnr_history.append(psnr.item())

    <span class="kw">return</span> model, psnr_history
      </code></pre>

      <div class="walkthrough">
        <h4>Walkthrough</h4>
        <p>
          This training loop ties together rays, samples, the NeRF MLP, and the volume renderer into a single optimization problem.
        </p>
        <ol>
          <li><strong>Ray precomputation:</strong> I build a pinhole intrinsics matrix from the Lego focal length, then unproject every pixel of every view into a world-space ray using <code>pixel_to_ray</code>.</li>
          <li><strong>Dataset flattening:</strong> All ray origins, ray directions, and RGB colors are flattened so each index corresponds to a single ray–pixel pair.</li>
          <li><strong>Mini-batch sampling:</strong> At each iteration I randomly choose <code>rays_per_batch</code> rays, sample 3D points along them, and query the NeRF network.</li>
          <li><strong>Rendering:</strong> <code>volume_render</code> integrates densities and colors along each ray to produce a batch of predicted pixel colors.</li>
          <li><strong>Loss + PSNR:</strong> I minimize MSE between rendered colors and ground-truth pixels using Adam, and track PSNR to monitor how quickly the model is fitting the multi-view data.</li>
        </ol>
      </div>

      <div class="grid grid-3">
        <figure>
          <img src="images/part2_lego_nerf/train_lego_iter0000.png" alt="Lego NeRF training iteration 0" />
          <figcaption>Lego NeRF at iteration 0 – essentially noise.</figcaption>
        </figure>
        <figure>
          <img src="images/part2_lego_nerf/train_lego_iter0200.png" alt="Lego NeRF training iteration 200" />
          <figcaption>Iteration 200 – rough geometry visible.</figcaption>
        </figure>
        <figure>
          <img src="images/part2_lego_nerf/train_lego_iter1000.png" alt="Lego NeRF training iteration 1000" />
          <figcaption>Iteration 1000 – refined textures and clear object boundaries.</figcaption>
        </figure>
      </div>

      <figure class="wide">
        <img src="images/part2_lego_nerf/psnr_curve_lego.png" alt="PSNR curve for Lego NeRF validation set" />
        <figcaption>PSNR vs training iteration for the Lego scene. The curve stabilizes once the network fits all views consistently.</figcaption>
      </figure>

      <figure class="wide">
        <img src="images/part2_lego_nerf/loss_curve_lego.png" alt="Loss curve for Lego NeRF validation set" />
        <figcaption>Loss vs training iteration for the Lego scene.</figcaption>
      </figure>

      <h3>2.5.1 Spherical Novel-View Rendering</h3>
      <p>
        To demonstrate true 3D understanding, I render novel views of the Lego scene from a spherical trajectory around the object.
        The NeRF is never explicitly told about these views; it synthesizes them from the learned radiance field.
      </p>

      <div class="grid grid-3">
        <figure>
          <img src="images/part2_lego_nerf/lego_spherical_frame00.png" alt="Lego spherical rendering frame 0" />
          <figcaption>Frame 0 – initial viewpoint from the training set’s general region.</figcaption>
        </figure>
        <figure>
          <img src="images/part2_lego_nerf/lego_spherical_frame20.png" alt="Lego spherical rendering frame 20" />
          <figcaption>Frame 20 – midway through the orbit, revealing previously occluded parts.</figcaption>
        </figure>
        <figure>
          <img src="images/part2_lego_nerf/lego_spherical_frame40.png" alt="Lego spherical rendering frame 40" />
          <figcaption>Frame 40 – later in the orbit, showing consistent geometry and appearance.</figcaption>
        </figure>
      </div>

      <figure class="wide">
        <img src="images/part2_lego_nerf/lego_spherical.gif" alt="Lego spherical orbit GIF" />
        <figcaption>Full spherical orbit GIF for the Lego scene, illustrating smooth novel views around the entire object.</figcaption>
      </figure>

      <p class="callout">
        <span class="label">Takeaway</span><br />
        The Lego experiment demonstrates that NeRF can recover a coherent 3D representation purely from multiple images and camera
        poses, without any explicit 3D supervision. Everything emerges from minimizing reconstruction error across views.
      </p>
    </section>

    <!-- Part 2.6: My Own Object NeRF -->
    <section id="part26">
      <h2>Part 2.6 – Training with My Own Object Data (2.6.1–2.6.4)</h2>
      <p>
        Finally, I apply the entire NeRF pipeline to the dataset I captured in Part 0, training a NeRF that can synthesize novel
        views of my own object.
      </p>

      <h3>2.6.1 Dataset & Preprocessing</h3>
      <p>
        I use the undistorted images and <code>c2w</code> matrices produced earlier and package them into
        <code>images_train</code>, <code>images_val</code>, <code>c2ws_train</code>, and <code>c2ws_val</code>. I slightly adjust
        near/far bounds and number of samples to match the physical size of my scene (e.g., <code>near ≈ 0.02</code>,
        <code>far ≈ 0.5</code>, 64 samples per ray).
      </p>

      <h3>2.6.2 Training Behavior</h3>
      <p>
        Training behavior is similar to Lego but a bit more sensitive to hyperparameters, since my capture is less “perfect” than
        the synthetic dataset. The loss curve below shows the training loss decreasing over time.
      </p>

      <figure class="wide">
        <img src="images/part2_6_your_object/object_loss_curve.png" alt="Training loss curve for my object NeRF" />
        <figcaption>Training loss over iterations for my own object NeRF.</figcaption>
      </figure>

      <div class="grid grid-4">
        <figure>
          <img src="images/part2_6_your_object/object_intermediate_iter050.png" alt="NeRF of my object, iteration 50" />
          <figcaption>Iteration 50 – noisy but coarse silhouette is visible.</figcaption>
        </figure>
        <figure>
          <img src="images/part2_6_your_object/object_intermediate_iter200.png" alt="NeRF of my object, iteration 200" />
          <figcaption>Iteration 200 – main geometry emerges.</figcaption>
        </figure>
        <figure>
          <img src="images/part2_6_your_object/object_intermediate_iter500.png" alt="NeRF of my object, iteration 500" />
          <figcaption>Iteration 500 – textures sharpening.</figcaption>
        </figure>
        <figure>
          <img src="images/part2_6_your_object/object_intermediate_iter1000.png" alt="NeRF of my object, iteration 1000" />
          <figcaption>Iteration 1000 – final reconstruction.</figcaption>
        </figure>
      </div>

      <h3>2.6.3 Novel View Animation</h3>
      <p>
        I synthesize a small orbiting camera path around the object by creating new <code>c2w</code> matrices that place the camera
        on a circle, always looking at the origin. For each frame, I render an image using the trained NeRF and combine them into a GIF.
      </p>

      <div class="grid grid-3">
        <figure>
          <img src="images/part2_6_your_object/object_spherical_frame01.png" alt="My object spherical rendering frame 1" />
          <figcaption>Novel view frame 1.</figcaption>
        </figure>
        <figure>
          <img src="images/part2_6_your_object/object_spherical_frame02.png" alt="My object spherical rendering frame 2" />
          <figcaption>Novel view frame 2.</figcaption>
        </figure>
        <figure>
          <img src="images/part2_6_your_object/object_spherical_frame03.png" alt="My object spherical rendering frame 3" />
          <figcaption>Novel view frame 3.</figcaption>
        </figure>
      </div>

      <figure class="wide">
        <img src="images/part2_6_your_object/object_spherical.gif" alt="My object orbit GIF" />
        <figcaption>Full orbit GIF for my object, showing consistent geometry across viewpoints.</figcaption>
      </figure>

      <h3>2.6.4 Reflection</h3>
      <p>
        Compared to the Lego scene, my own data is noisier, less uniformly lit, and has fewer views. Nonetheless, NeRF is able to
        reconstruct a coherent 3D model that produces convincing novel views. This highlights both the power and fragility of
        the method: it can interpolate impressively, but is sensitive to calibration quality, coverage, and exposure consistency.
      </p>
    </section>

    <!-- Implementation Notes -->
    <section id="impl-notes" class="compact">
      <h2>Implementation Notes & Pitfalls</h2>
      <p>
        This section summarizes key parameter choices and practical issues I encountered while implementing NeRF from scratch.
      </p>
      <ul>
        <li><strong>Batch size:</strong> I used around 10k rays per iteration for Lego. Larger batches stabilize training but increase GPU memory usage.</li>
        <li><strong>Near/Far bounds:</strong> Getting these wrong either wastes samples (too wide) or chops off geometry (too tight).
          For my object I tuned <code>near</code> and <code>far</code> by visualizing depth and trying a few ranges.</li>
        <li><strong>Positional encoding frequencies:</strong> Very high frequencies can cause ringing artifacts if the network or data
          can’t support them. I found <code>pos_freqs≈10</code>, <code>dir_freqs≈4</code> to be a good balance.</li>
        <li><strong>Learning rate:</strong> For Lego I used <code>5e-4</code> with Adam. Higher learning rates converged faster but risked
          oscillations in PSNR.</li>
        <li><strong>Data quality:</strong> Slight miscalibration (especially in intrinsics) can produce subtle ghosting in the final NeRF.
          Carefully checking the Viser camera cloud was essential before committing to long training runs.</li>
        <li><strong>Debugging strategy:</strong> I verified each stage (ray directions, sampling locations, volume rendering) with small,
          synthetic tests before combining them. This helped isolate bugs that would otherwise manifest as “blurry renderings.”</li>
      </ul>

      <p class="callout">
        <span class="label">Lessons Learned</span><br />
        NeRF looks intimidating because of the integral in the volume rendering equation, but in code it’s mostly linear algebra and
        exponentials. The hard part isn’t the math—it’s keeping every coordinate system, normalization, and range consistent across
        the entire pipeline.
      </p>
    </section>

    <!-- References -->
    <section id="references" class="compact">
      <h2>References</h2>
      <ul>
        <li>
          Mildenhall et al., <em>NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis</em>, ECCV 2020.
        </li>
        <li>
          CS180: Intro to Computer Vision and Computational Photography – Project 4 spec and starter code.
        </li>
        <li>
          OpenCV documentation: camera calibration, ArUco detection, and PnP pose estimation.
        </li>
        <li>
          PyTorch documentation for building and training neural networks on GPU.
        </li>
      </ul>
    </section>
  </main>

  <footer>
    <p>© 2025 · CS180 Project 4 – Neural Radiance Fields.</p>
    <p>
      Some explanatory text and structure were drafted with the assistance of an AI model (ChatGPT, OpenAI) and then reviewed
      and edited by me. All implementation decisions, experiments, and final interpretations are my own.
    </p>
  </footer>
</body>
</html>
