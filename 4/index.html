<!DOCTYPE html> 
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>CS180 Project 4 – Neural Radiance Fields (NeRF) https://eduardo1100.github.io/CS180/4/index.html#impl-notes</title>
  <meta name="description" content="CS180 Project 4 – Neural Radiance Fields: camera calibration, 2D neural fields, Lego NeRF, and NeRF on my own object." />

  <!-- Inter Font -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;800&display=swap" rel="stylesheet">

  <style>
    :root {
      --bg: #070b11;
      --bg-elevated: #0e141f;
      --bg-soft: #141b28;
      --ink: #e6edf7;
      --ink-muted: #9aa4c6;
      --accent: #6ab8ff;
      --accent-soft: rgba(106, 184, 255, 0.12);
      --border-subtle: #222b3c;
      --code-bg: #0b101a;
      --code-border: #222a3a;
      --callout-bg: #101827;
      --callout-border: #39567f;
      --danger: #ff6b7a;
      --success: #6bffb4;
      --max-width: 1100px;
      --radius-lg: 18px;
      --radius-sm: 10px;
      --shadow-soft: 0 22px 45px rgba(0, 0, 0, 0.55);
      --shadow-subtle: 0 16px 35px rgba(0, 0, 0, 0.35);
      --nav-height: 64px;
    }

    * { box-sizing: border-box; }
    html { scroll-behavior: smooth; }
    body {
      margin: 0;
      background: radial-gradient(circle at top, #181d2b 0, #05060a 55%, #020308 100%);
      color: var(--ink);
      font-family: "Inter", system-ui, -apple-system, BlinkMacSystemFont, sans-serif;
      line-height: 1.7;
      -webkit-font-smoothing: antialiased;
    }
    a { color: var(--accent); text-decoration: none; }
    a:hover { text-decoration: underline; }

    /* Top Navigation */
    header.top-nav {
      position: sticky; top: 0; z-index: 100; backdrop-filter: blur(18px);
      background: linear-gradient(to right, rgba(3,6,12,0.94), rgba(4,8,16,0.9));
      border-bottom: 1px solid rgba(255,255,255,0.04);
    }
    .nav-inner { max-width: var(--max-width); margin: 0 auto; padding: 12px 20px; display: flex; align-items: center; justify-content: space-between; gap: 16px; }
    .nav-title { font-size: 0.95rem; font-weight: 600; letter-spacing: 0.08em; text-transform: uppercase; color: var(--ink-muted); white-space: nowrap; }
    .nav-links { display: flex; flex-wrap: wrap; gap: 10px; font-size: 0.85rem; }
    .nav-links a { padding: 6px 10px; border-radius: 999px; border: 1px solid transparent; color: var(--ink-muted); background: transparent; transition: all 0.15s ease; }
    .nav-links a:hover { border-color: rgba(255,255,255,0.07); background: rgba(255,255,255,0.03); color: var(--ink); text-decoration: none; }
    .nav-links a.primary-link { border-color: rgba(255,255,255,0.08); background: radial-gradient(circle at top left, rgba(106,184,255,0.14), rgba(15,22,40,0.8)); color: var(--ink); }

    /* Layout */
    main { max-width: var(--max-width); margin: 0 auto; padding: 30px 18px 60px; }
    section { margin-bottom: 52px; padding: 26px 22px 28px; border-radius: var(--radius-lg); background: linear-gradient(145deg, rgba(11,16,27,0.98), rgba(5,7,13,0.98)); box-shadow: var(--shadow-soft); border: 1px solid rgba(255,255,255,0.02); }
    section.compact { margin-bottom: 30px; padding: 18px 18px 20px; box-shadow: var(--shadow-subtle); }
    h1, h2, h3, h4 { margin-top: 0; color: var(--ink); }
    h1 { font-size: 2.1rem; letter-spacing: 0.03em; text-transform: uppercase; font-weight: 800; margin-bottom: 0.5rem; }
    h2 { font-size: 1.4rem; margin-bottom: 0.4rem; border-bottom: 1px solid rgba(255,255,255,0.06); padding-bottom: 0.25rem; }
    h3 { font-size: 1.15rem; margin-top: 1.4rem; margin-bottom: 0.35rem; }
    h4 { font-size: 1.0rem; margin-top: 1.1rem; margin-bottom: 0.25rem; color: var(--ink-muted); text-transform: uppercase; letter-spacing: 0.08em; }
    p { margin-top: 0.4rem; margin-bottom: 0.6rem; color: var(--ink-muted); font-size: 0.96rem; }
    p.lead { font-size: 1.02rem; color: var(--ink); max-width: 80ch; }
    p.callout { margin-top: 0.8rem; margin-bottom: 0.8rem; padding: 12px 14px; border-radius: var(--radius-sm); background: var(--callout-bg); border: 1px solid var(--callout-border); color: var(--ink); font-size: 0.94rem; }
    p.callout span.label { font-size: 0.78rem; text-transform: uppercase; letter-spacing: 0.12em; color: var(--accent); display: inline-block; margin-bottom: 4px; }
    ul, ol { margin-top: 0.4rem; margin-bottom: 0.8rem; padding-left: 1.2rem; color: var(--ink-muted); font-size: 0.95rem; }
    li + li { margin-top: 0.18rem; }
    strong { color: var(--ink); }

    /* Grids & Figures */
    .grid { display: grid; gap: 14px; margin: 0.6rem 0 1.0rem; }
    .grid-2 { grid-template-columns: repeat(auto-fit, minmax(230px, 1fr)); }
    .grid-3 { grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); }
    .grid-4 { grid-template-columns: repeat(auto-fit, minmax(150px, 1fr)); }
    figure { margin: 0; padding: 10px; border-radius: 14px; background: var(--bg-soft); border: 1px solid rgba(255,255,255,0.05); box-shadow: 0 10px 25px rgba(0,0,0,0.35); }
    figure.wide { grid-column: 1 / -1; }
    figure img { display: block; width: 100%; border-radius: 10px; }
    figcaption { margin-top: 6px; font-size: 0.83rem; color: var(--ink-muted); }

    /* Code Blocks */
    pre.code { margin: 0.8rem 0 0.4rem; padding: 12px 14px; border-radius: var(--radius-sm); background: radial-gradient(circle at top left, #121b30, #050811); border: 1px solid var(--code-border); overflow-x: auto; box-shadow: 0 12px 28px rgba(0,0,0,0.55); }
    pre.code code { font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, "Cascadia Code", "Fira Code", monospace; font-size: 0.86rem; color: #f3f4ff; }

    .walkthrough { margin: 0.3rem 0 0.9rem; padding: 10px 12px 6px; border-left: 2px solid rgba(255,255,255,0.12); background: linear-gradient(to right, rgba(255,255,255,0.02), transparent); border-radius: 0 10px 10px 0; }
    .walkthrough p { font-size: 0.93rem; }
    .walkthrough ol { font-size: 0.9rem; }

    .meta-row { display: flex; flex-wrap: wrap; align-items: center; gap: 8px; margin-bottom: 0.4rem; font-size: 0.8rem; color: var(--ink-muted); text-transform: uppercase; letter-spacing: 0.14em; }
    .meta-pill { padding: 4px 8px; border-radius: 999px; border: 1px solid rgba(255,255,255,0.08); background: rgba(255,255,255,0.02); }

    footer { max-width: var(--max-width); margin: 0 auto 32px; padding: 14px 18px 6px; border-radius: var(--radius-lg); border: 1px solid rgba(255,255,255,0.04); background: radial-gradient(circle at top left, rgba(255,255,255,0.03), rgba(1,2,5,0.98)); font-size: 0.8rem; color: var(--ink-muted); }
    footer p { margin: 4px 0; }
    @media (max-width: 720px) { .nav-inner { padding-inline: 14px; } main { padding-inline: 14px; } section { padding-inline: 16px; } h1 { font-size: 1.7rem; } .nav-title { display: none; } }
  </style>
</head>

<body>
  <!-- Sticky Navigation -->
  <header class="top-nav">
    <div class="nav-inner">
      <div class="nav-title">CS180 · Project 4 · Neural Radiance Fields</div>
      <nav class="nav-links">
        <a href="#overview" class="primary-link">Overview</a>
        <a href="#part0">Part 0 – Capture</a>
        <a href="#partA">Part A – 2D Neural Field</a>
        <a href="#partB">Part B – Lego NeRF</a>
        <a href="#partC">Part C – My Object NeRF</a>
        <a href="#impl-notes">Implementation Notes</a>
        <a href="#references">References</a>
      </nav>
    </div>
  </header>

  <main>
    <!-- Overview -->
    <section id="overview">
      <div class="meta-row">
        <span class="meta-pill">CS180 · Intro to Computer Vision and Computational Photography</span>
        <span class="meta-pill">Fall 2025</span>
        <span class="meta-pill">Neural Radiance Fields</span>
      </div>
      <h1>Project 4 – Neural Radiance Fields (NeRF)</h1>
      <p class="lead">
        In this project I go from raw phone images of a small object to a fully learned, continuous 3D representation using
        <strong>Neural Radiance Fields (NeRF)</strong>. The pipeline includes camera calibration and pose estimation, fitting a
        2D neural field on an image, training a full NeRF on the classic Lego dataset, and finally learning a NeRF of my own object.
      </p>
      <p class="callout">
        <span class="label">High-level Intuition</span><br />
        Rather than storing a 3D model as meshes or voxels, NeRF learns a <em>continuous function</em> that, given a point in 3D space
        and a viewing direction, predicts color and density. By integrating these predictions along camera rays, we can render
        realistic images from new viewpoints.
      </p>
    </section>

    <!-- Part 0: Camera Calibration & 3D Capture -->
    <section id="part0">
      <h2>Part 0 – Camera Calibration & 3D Capture</h2>
      <p>
        Before training any NeRF, we need accurate camera parameters. This part covers:
      </p>
      <ul>
        <li><strong>0.1 – Camera calibration</strong> with ArUco tags to recover intrinsics.</li>
        <li><strong>0.2 – Object capture</strong> with consistent lighting and distance.</li>
        <li><strong>0.3 – Pose estimation</strong> using Perspective-n-Point (PnP).</li>
        <li><strong>0.4 – Undistortion and dataset packaging</strong> into the NeRF-ready <code>.npz</code> format.</li>
      </ul>

      <h3>0.1 Camera Calibration with ArUco Tags</h3>
      <p>
        I printed an ArUco tag grid and captured 30–50 images from different angles while keeping the phone’s focal length fixed.
        OpenCV detects the corners in 2D, which I associate with known 3D points on the flat tag. From those correspondences,
        <code>cv2.calibrateCamera</code> estimates the camera intrinsics matrix and lens distortion.
      </p>

      <pre class="code"><code>
import os
import glob
import numpy as np
import cv2

TAG_SIZE_M = 0.02

def tag_corners_3d(tag_size_m=TAG_SIZE_M):
    s = float(tag_size_m)
    return np.array([
        [0.0, 0.0, 0.0],
        [s,   0.0, 0.0],
        [s,   s,   0.0],
        [0.0, s,   0.0],
    ], dtype=np.float32)

def create_aruco_detector():
    aruco_dict = cv2.aruco.getPredefinedDictionary(cv2.aruco.DICT_4X4_50)
    aruco_params = cv2.aruco.DetectorParameters()
    return aruco_dict, aruco_params

def run_part0_calibration(calib_dir, out_path):
    image_paths = sorted(glob.glob(os.path.join(calib_dir, "*.jpg")))
    aruco_dict, aruco_params = create_aruco_detector()

    objpoints, imgpoints = [], []
    image_size = None

    for path in image_paths:
        img = cv2.imread(path)
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        image_size = (gray.shape[1], gray.shape[0])

        corners, ids, _ = cv2.aruco.detectMarkers(gray, aruco_dict, parameters=aruco_params)
        if ids is None or len(corners) == 0:
            continue

        img_corners = corners[0].reshape(-1, 2).astype(np.float32)
        imgpoints.append(img_corners)
        objpoints.append(tag_corners_3d())

    objpoints = [op.reshape(-1, 1, 3) for op in objpoints]
    imgpoints = [ip.reshape(-1, 1, 2) for ip in imgpoints]

    ret, K, dist_coeffs, rvecs, tvecs = cv2.calibrateCamera(
        objpoints, imgpoints, image_size, None, None
    )

    np.savez(out_path, K=K, dist_coeffs=dist_coeffs, image_size=np.array(image_size))
    return K, dist_coeffs, image_size
      </code></pre>

      <div class="walkthrough">
        <h4>Walkthrough</h4>
        <p>
          The goal here is to translate the visual pattern on the ArUco grid into numerical constraints the solver can use to
          infer the camera’s focal length and principal point.
        </p>
        <ol>
          <li><strong>Tag geometry:</strong> <code>tag_corners_3d</code> encodes the 2 cm edge length of the printed tag into a consistent 3D coordinate frame on the <code>z = 0</code> plane. This scale propagates all the way into NeRF.</li>
          <li><strong>Detector setup:</strong> <code>create_aruco_detector</code> fixes the marker family and parameters so that detection behavior is reproducible across runs.</li>
          <li><strong>Image loop:</strong> For each calibration image, I read it with OpenCV, convert to grayscale, and detect markers. If a frame has no detections, I skip it so a few bad photos don’t hurt calibration.</li>
          <li><strong>2D–3D correspondences:</strong> I take the first detected tag’s corners as 2D image points and pair them with the canonical 3D corners for that tag. <code>calibrateCamera</code> expects them reshaped into <code>(N,1,·)</code> arrays.</li>
          <li><strong>Solving:</strong> <code>calibrateCamera</code> returns the intrinsic matrix <code>K</code> and distortion coefficients that define my camera model. I save everything to an <code>.npz</code> file so later scripts can reuse the results without re-running calibration.</li>
        </ol>
      </div>

      <h3>0.2 Object Capture</h3>
      <p>
        I chose a small object, placed a single ArUco tag on the table next to it, and captured 30–50 photos while moving the camera
        in an arc around the object. I tried to keep:
      </p>
      <ul>
        <li><strong>Exposure</strong> roughly constant (no automatic brightness jumps).</li>
        <li><strong>Distance</strong> such that the object occupies ~50% of the frame.</li>
        <li><strong>Blur</strong> minimal by holding the phone steady.</li>
      </ul>

      <!-- Removed: non-deliverable object capture example photos -->

      <h3>0.3 Pose Estimation and Viser Visualization</h3>
      <p>
        Using the intrinsics and distortion coefficients from calibration, I estimate the camera pose for each object image with
        <code>cv2.solvePnP</code>. This gives me the camera’s rotation and translation relative to the tag, which I convert into a
        camera-to-world matrix (<code>c2w</code>). I then visualize all the camera frustums in 3D using <code>viser</code>.
      </p>

      <div class="grid grid-2">
        <figure class="wide">
          <img src="images/part0_camera_calibration/viser_frustum_view1.png" alt="Viser visualization of camera frustums – view 1" />
          <figcaption>Viser visualization of all camera frustums orbiting the object (view 1).</figcaption>
        </figure>
        <figure class="wide">
          <img src="images/part0_camera_calibration/viser_frustum_view2.png" alt="Viser visualization of camera frustums – view 2" />
          <figcaption>Another viewpoint of the same cloud of cameras, confirming a reasonably smooth arc.</figcaption>
        </figure>
      </div>

      <h3>0.4 Undistortion & Dataset Packaging</h3>
      <p>
        NeRF assumes a simple pinhole camera model without lens distortion, so I undistort every image and crop valid pixels using
        <code>cv2.getOptimalNewCameraMatrix</code>. I then build a <code>.npz</code> containing images and their corresponding
        <code>c2w</code> matrices, split into train/val/test.
      </p>

      <!-- Removed: non-deliverable undistort before/after images -->
    </section>

    <!-- Part A: 2D Neural Field -->
    <section id="partA">
      <h2>Part A – 2D Neural Field (A.1–A.4)</h2>
      <p>
        Before tackling full 3D NeRFs, I first train a <strong>2D neural field</strong> that maps pixel coordinates to colors in a single
        image. This is an easier sandbox to understand positional encoding, MLP architecture, and training behavior.
      </p>

      <h3>A.1 Objective & Intuition</h3>
      <p>
        The neural field is a function <code>F(u, v) → RGB</code> that takes continuous, normalized pixel coordinates and outputs
        the color at that point. Instead of storing the image as a grid of values, I store it as the weights of a neural network—
        a kind of compressed, continuous representation.
      </p>

      <h3>A.2 Network & Positional Encoding</h3>
      <p>
        I use a small MLP with sinusoidal positional encoding (PE). PE expands coordinates into a higher-dimensional space using
        sines and cosines, enabling the network to capture fine details and edges.
      </p>

      <pre class="code"><code>
import math
import torch
import torch.nn as nn

class PosEnc(nn.Module):
    def __init__(self, num_freqs=10):
        super().__init__()
        self.num_freqs = num_freqs

    def forward(self, x):
        encodings = [x]
        for i in range(self.num_freqs):
            freq = 2.0 ** i * math.pi
            encodings.append(torch.sin(freq * x))
            encodings.append(torch.cos(freq * x))
        return torch.cat(encodings, dim=-1)

class NeuralField2D(nn.Module):
    def __init__(self, width=128, num_freqs=10):
        super().__init__()
        self.pe = PosEnc(num_freqs)
        in_dim = 2 + 2 * 2 * num_freqs

        layers = []
        hidden_dims = [width] * 4
        last_dim = in_dim
        for h in hidden_dims:
            layers.append(nn.Linear(last_dim, h))
            layers.append(nn.ReLU(inplace=True))
            last_dim = h

        self.mlp = nn.Sequential(*layers)
        self.out_layer = nn.Sequential(
            nn.Linear(last_dim, 3),
            nn.Sigmoid(),
        )

    def forward(self, uv):
        x = self.pe(uv)
        h = self.mlp(x)
        rgb = self.out_layer(h)
        return rgb
      </code></pre>

      <div class="walkthrough">
        <h4>Walkthrough</h4>
        <p>
          This block defines the core 2D neural field model. It hides most of the math of “fitting a function to an image”
          inside a simple PyTorch module.
        </p>
        <ol>
          <li><strong><code>PosEnc</code>:</strong> The positional encoding layer takes raw <code>(u, v)</code> coordinates in <code>[0, 1]</code> and builds a richer feature vector using sine and cosine at exponentially increasing frequencies. This lets the MLP represent both smooth regions and sharp edges.</li>
          <li><strong>Frequency loop:</strong> For each <code>i</code>, I compute <code>freq = 2^i · π</code>. Applying <code>sin(freq · x)</code> and <code>cos(freq · x)</code> at multiple frequencies effectively creates a Fourier-like basis over coordinates.</li>
          <li><strong>Input dimension:</strong> The MLP input has the original 2 coordinates plus <code>2 · (sin, cos) · num_freqs</code> values per coordinate. Concatenating all of these yields a high-dimensional input describing the position.</li>
          <li><strong>MLP architecture:</strong> I use 4 fully connected layers with ReLU activations. This is enough capacity to memorize a moderate-resolution image without being too slow.</li>
          <li><strong>Output layer:</strong> The final <code>Linear → Sigmoid</code> block maps to three channels in <code>[0, 1]</code>, which correspond directly to RGB values.</li>
        </ol>
      </div>

      <h3>A.3 Training & PSNR</h3>
      <p>
        During training, I randomly sample 10k pixels at each iteration, feed their normalized coordinates into the network,
        and compare predicted colors against ground truth using mean squared error (MSE). I track reconstruction quality using
        PSNR (Peak Signal-to-Noise Ratio).
      </p>

      <div class="grid grid-3">
        <figure>
          <img src="images/part1_2d_neural_field/train_provided_iter0000.png" alt="Training progression on provided image, iteration 0" />
          <figcaption>Provided image: random initialization (iteration 0).</figcaption>
        </figure>
        <figure>
          <img src="images/part1_2d_neural_field/train_provided_iter0300.png" alt="Training progression on provided image, iteration 300" />
          <figcaption>Provided image: structure emerging (iteration 300).</figcaption>
        </figure>
        <figure>
          <img src="images/part1_2d_neural_field/train_provided_iter2000.png" alt="Training progression on provided image, final iteration" />
          <figcaption>Provided image: final reconstruction closely matches the original (iteration 2000).</figcaption>
        </figure>
      </div>

      <div class="grid grid-3">
        <figure>
          <img src="images/part1_2d_neural_field/train_own_iter0000.png" alt="Training progression on my own image, iteration 0" />
          <figcaption>My own image: random initialization (iteration 0).</figcaption>
        </figure>
        <figure>
          <img src="images/part1_2d_neural_field/train_own_iter0300.png" alt="Training progression on my own image, iteration 300" />
          <figcaption>My own image: major structure appears within a few hundred steps (iteration 300).</figcaption>
        </figure>
        <figure>
          <img src="images/part1_2d_neural_field/train_own_iter2000.png" alt="Training progression on my own image, final iteration" />
          <figcaption>My own image: final neural reconstruction (iteration 2000).</figcaption>
        </figure>
      </div>

      <figure class="wide">
        <img src="images/part1_2d_neural_field/psnr_curve_own.png" alt="PSNR curve over training iterations for the 2D neural field" />
        <figcaption>PSNR vs iteration for the 2D neural field. The curve plateaus when the network has fully memorized the image.</figcaption>
      </figure>

      <h3>A.4 Hyperparameter Sweeps</h3>
      <p>
        I sweep over two key hyperparameters: the width of the MLP and the maximum PE frequency <code>L</code>. The following grid
        shows how capacity and frequency content affect sharpness:
      </p>

      <div class="grid grid-2">
        <figure>
          <img src="images/part1_2d_neural_field/train_grid_L4_W64_iter1000.png" alt="Low frequency, low width" />
          <figcaption>Low PE frequency = 4, low width = 64: overly smooth, blurred edges.</figcaption>
        </figure>
        <figure>
          <img src="images/part1_2d_neural_field/train_grid_L4_W128_iter1000.png" alt="Low frequency, high width" />
          <figcaption>Low PE frequency = 4, high width = 128: more capacity but still lacking fine detail.</figcaption>
        </figure>
        <figure>
          <img src="images/part1_2d_neural_field/train_grid_L10_W64_iter1000.png" alt="High frequency, low width" />
          <figcaption>High PE frequency = 10, low width = 64: some high-frequency detail, but underfit in complex regions.</figcaption>
        </figure>
        <figure>
          <img src="images/part1_2d_neural_field/train_grid_L10_W128_iter1000.png" alt="High frequency, high width" />
          <figcaption>High PE frequency = 10, high width = 128: sharpest reconstruction, but with higher computational cost.</figcaption>
        </figure>
      </div>

      <p class="callout">
        <span class="label">Takeaway</span><br />
        Positional encoding is what allows the MLP to represent crisp edges and small details, while width controls how much capacity
        the network has to memorize complex textures. Together, they determine the trade-off between smoothness and fidelity.
      </p>
    </section>

    <!-- Part B: Lego NeRF -->
    <section id="partB">
      <h2>Part B – Lego NeRF (B.1–B.5)</h2>
      <p>
        With 2D neural fields working, I move to the full 3D NeRF setup on the classic Lego dataset. Here the model takes 3D points
        and viewing directions as input and predicts both density and color, which are combined using volume rendering.
      </p>

      <h3>B.1 Rays from Cameras</h3>
      <p>
        I first convert pixel coordinates into 3D rays using the camera intrinsics and camera-to-world matrices. Each ray has an
        origin (camera center) and a direction in world space.
      </p>

      <pre class="code"><code>
import numpy as np

def get_rays(H, W, focal, c2w):
    fx = fy = float(focal)
    cx, cy = W * 0.5, H * 0.5

    u = np.arange(W)
    v = np.arange(H)
    uu, vv = np.meshgrid(u, v, indexing="xy")

    x = (uu + 0.5 - cx) / fx
    y = (vv + 0.5 - cy) / fy
    z = np.ones_like(x, dtype=np.float32)

    dirs_cam = np.stack([x, y, z], axis=-1)
    dirs_cam = dirs_cam / (np.linalg.norm(dirs_cam, axis=-1, keepdims=True) + 1e-9)

    R = c2w[:3, :3]
    t = c2w[:3, 3]

    dirs_world = (R @ dirs_cam.reshape(-1, 3).T).T
    dirs_world = dirs_world / (np.linalg.norm(dirs_world, axis=-1, keepdims=True) + 1e-9)
    dirs_world = dirs_world.reshape(H, W, 3)

    origins = np.broadcast_to(t.reshape(1, 1, 3), (H, W, 3)).copy()

    return origins.astype(np.float32), dirs_world.astype(np.float32)
      </code></pre>

      <div class="walkthrough">
        <h4>Walkthrough</h4>
        <p>
          This function is the precise implementation I use everywhere (training, evaluation, and visualizations) to map pixels to 3D rays.
        </p>
        <ol>
          <li><strong>Pixel grid:</strong> I build a grid of integer pixel indices using <code>np.meshgrid</code> with <code>indexing="xy"</code>, which matches the convention used by my <code>RaysData</code> dataset class.</li>
          <li><strong>Camera-frame directions:</strong> The pinhole model converts pixel offsets from the principal point <code>(cx, cy)</code> into normalized directions in camera space, with <code>+Z</code> as the forward direction.</li>
          <li><strong>Normalization:</strong> I explicitly normalize the direction vectors to unit length to avoid scale issues when sampling along the ray.</li>
          <li><strong>World-frame transform:</strong> Multiplying by the 3×3 block of <code>c2w</code> and adding the translation gives the ray directions and origins in world coordinates. The camera center is the same for every pixel, so I broadcast <code>t</code> over the grid.</li>
          <li><strong>Consistency:</strong> Because <code>get_rays</code> is used both for training batches and for full-image validation renders, any subtle choice (like pixel-center offsets or which axis is forward) is shared across the entire pipeline.</li>
        </ol>
      </div>

      <h3>B.2 Sampling Points along Rays</h3>
      <p>
        For each ray, I sample a set of points between a near and far bound (2.0 and 6.0 for the Lego scene). During training,
        I add small random perturbations to encourage the model to cover the entire interval and avoid overfitting to a fixed grid.
      </p>

      <pre class="code"><code>
import torch

def sample_along_rays(rays_o, rays_d, n_samples=64, near=2.0, far=6.0, perturb=True):
    device_ = rays_o.device
    B = rays_o.shape[0]

    t_vals = torch.linspace(near, far, n_samples, device=device_)
    t_vals = t_vals.expand(B, n_samples)

    if perturb:
        mids = 0.5 * (t_vals[:, :-1] + t_vals[:, 1:])
        widths = t_vals[:, 1:] - t_vals[:, :-1]
        noise = (torch.rand_like(mids) - 0.5) * widths
        t_vals = torch.cat([mids + noise, t_vals[:, -1:]], dim=-1)

    pts = rays_o[..., None, :] + rays_d[..., None, :] * t_vals[..., None]
    return pts, t_vals
      </code></pre>

      <div class="walkthrough">
        <h4>Walkthrough</h4>
        <p>
          NeRF is essentially integrating along each ray, so we approximate that integral by sampling discrete points.
        </p>
        <ol>
          <li><strong>Shared depth grid:</strong> I start with <code>torch.linspace(near, far, n_samples)</code> to get evenly spaced depths along the ray interval. This is expanded to all rays in the batch.</li>
          <li><strong>Device awareness:</strong> The samples are created directly on <code>rays_o.device</code>, which lets this function work seamlessly on both CPU and GPU without extra copies.</li>
          <li><strong>Perturbation:</strong> When <code>perturb=True</code>, I jitter the samples inside each interval using uniform noise. This behaves like anti-aliasing and reduces banding artifacts in the final render.</li>
          <li><strong>3D points:</strong> The formula <code>rays_o + t · rays_d</code> gives the 3D point at depth <code>t</code> along each ray. Broadcasting over <code>[B, N]</code> gives a dense grid of samples suitable for feeding into the NeRF MLP.</li>
          <li><strong>Output shapes:</strong> I return both the sampled 3D points and the scalar depth values, which are later needed by the volume renderer.</li>
        </ol>
      </div>

      <h3>B.3 NeRF Network Architecture</h3>
      <p>
        The NeRF network takes in 3D points and viewing directions, applies separate positional encodings, and outputs a density
        (scalar) and an RGB color conditioned on direction. The implementation below matches the 8-layer architecture from the
        spec, with a skip connection and a separate feature branch for color.
      </p>

      <pre class="code"><code>
import torch
import torch.nn as nn
import torch.nn.functional as F

class NeRF(nn.Module):
    def __init__(self, pos_freqs=10, dir_freqs=4, width=256):
        super().__init__()

        self.pos_pe = PosEnc(pos_freqs)
        self.dir_pe = PosEnc(dir_freqs)

        pos_dim = 3 + 2 * 3 * pos_freqs
        dir_dim = 3 + 2 * 3 * dir_freqs

        self.fc1 = nn.Linear(pos_dim, width)
        self.fc2 = nn.Linear(width,   width)
        self.fc3 = nn.Linear(width,   width)
        self.fc4 = nn.Linear(width,   width)

        self.fc5 = nn.Linear(width + pos_dim, width)
        self.fc6 = nn.Linear(width,          width)
        self.fc7 = nn.Linear(width,          width)
        self.fc8 = nn.Linear(width,          width)

        self.sigma_head = nn.Sequential(
            nn.Linear(width, 1),
            nn.ReLU(inplace=True),
        )

        self.fc_feat = nn.Sequential(
            nn.Linear(width, width),
            nn.ReLU(inplace=True),
        )

        self.fc_rgb = nn.Sequential(
            nn.Linear(width + dir_dim, 128),
            nn.ReLU(inplace=True),
            nn.Linear(128, 3),
            nn.Sigmoid(),
        )

    def forward(self, x, d):
        B, N, _ = x.shape

        x_flat = x.reshape(-1, 3)
        d_flat = d.reshape(-1, 3)

        x_enc = self.pos_pe(x_flat)
        d_enc = self.dir_pe(d_flat)

        h = F.relu(self.fc1(x_enc))
        h = F.relu(self.fc2(h))
        h = F.relu(self.fc3(h))
        h = F.relu(self.fc4(h))

        h = torch.cat([h, x_enc], dim=-1)

        h = F.relu(self.fc5(h))
        h = F.relu(self.fc6(h))
        h = F.relu(self.fc7(h))
        h = F.relu(self.fc8(h))

        sigma = self.sigma_head(h)

        feat = self.fc_feat(h)
        h_color = torch.cat([feat, d_enc], dim=-1)
        rgb = self.fc_rgb(h_color)

        sigma = sigma.view(B, N, 1)
        rgb   = rgb.view(B, N, 3)
        return sigma, rgb
      </code></pre>

      <div class="walkthrough">
        <h4>Walkthrough</h4>
        <p>
          This is the final NeRF architecture I use for both Lego and my own object. It closely follows the original paper and
          the project specification.
        </p>
        <ol>
          <li><strong>Shared positional encoders:</strong> I reuse the <code>PosEnc</code> module from Part A to encode both positions and view directions. Positions get more frequencies (<code>L = 10</code>) than directions (<code>L = 4</code>), since geometry is more spatially complex.</li>
          <li><strong>Deep position trunk:</strong> The first four fully connected layers operate only on the encoded position <code>PE(x)</code>, building a high-level representation of local geometry.</li>
          <li><strong>Skip connection:</strong> After layer 4, I concatenate the original positional encoding <code>PE(x)</code> back into the hidden representation. This helps gradients flow and lets the network preserve low-level spatial detail even in deep layers.</li>
          <li><strong>Second stage:</strong> Layers <code>fc5–fc8</code> refine this combined representation. This 8-layer trunk is deep enough to capture fine geometric structure across the scene.</li>
          <li><strong>Density head:</strong> The <code>sigma_head</code> maps the final hidden state to a single non-negative scalar <code>σ</code> per sample. Through volume rendering, this density controls where rays “stop.”</li>
          <li><strong>Color head:</strong> A separate feature branch feeds into the color head, where I concatenate the geometry features with the encoded view direction <code>PE(d)</code>. This enables view-dependent effects such as specular highlights.</li>
          <li><strong>Batch-friendly shapes:</strong> I treat <code>B × N</code> samples as a big flat batch inside the MLP, then reshape back to <code>[B, N, ·]</code> so the outputs line up with the sampled points along each ray.</li>
        </ol>
      </div>

      <h3>B.4 Volume Rendering</h3>
      <p>
        To render a pixel, I convert densities to opacities and integrate colors along the ray using the NeRF volume rendering equation.
        In discrete form, each sample contributes:
      </p>
      <p>
        <code>color = Σ T<sub>i</sub> · α<sub>i</sub> · c<sub>i</sub></code>, where <code>T<sub>i</sub></code> is the accumulated transmittance up to sample i,
        and <code>α<sub>i</sub> = 1 - exp(-σ<sub>i</sub> Δt)</code> is the opacity.
      </p>

      <pre class="code"><code>
import torch

def volume_render(sigmas, rgbs, t_vals_or_step):
    B, N, _ = sigmas.shape
    sigma = sigmas.squeeze(-1)

    if torch.is_tensor(t_vals_or_step) and t_vals_or_step.ndim == 2:
        t_vals = t_vals_or_step

        deltas = t_vals[:, 1:] - t_vals[:, :-1]
        last_delta = torch.full_like(deltas[:, :1], 1e10)
        deltas = torch.cat([deltas, last_delta], dim=-1)
    else:
        step_size = float(t_vals_or_step)
        deltas = torch.full(
            (B, N),
            step_size,
            device=sigmas.device,
            dtype=sigmas.dtype,
        )

    alpha = 1.0 - torch.exp(-sigma * deltas)

    T = torch.cumprod(
        torch.cat(
            [torch.ones(B, 1, device=sigma.device, dtype=sigma.dtype),
             1.0 - alpha + 1e-10],
            dim=-1,
        ),
        dim=-1,
    )[:, :-1]

    weights = T * alpha
    rgb_map = torch.sum(weights[..., None] * rgbs, dim=1)

    if torch.is_tensor(t_vals_or_step) and t_vals_or_step.ndim == 2:
        return rgb_map, weights
    else:
        return rgb_map
      </code></pre>

      <div class="walkthrough">
        <h4>Walkthrough</h4>
        <p>
          This function numerically approximates the continuous volume rendering integral using the discrete samples produced earlier.
        </p>
        <ol>
          <li><strong>Two call modes:</strong> For most of my training and evaluation I pass a full <code>t_vals</code> tensor, but I also support a scalar step size to match a reference test provided in the starter code.</li>
          <li><strong>Δt handling:</strong> When <code>t_vals</code> is provided, I compute per-interval distances and give the last sample a huge “catch-all” <code>Δt</code>, ensuring that any remaining density is absorbed at the end of the ray.</li>
          <li><strong>Opacity computation:</strong> Densities are converted into opacities via <code>α = 1 − exp(−σ Δt)</code>, which comes directly from the Beer–Lambert law for light attenuation.</li>
          <li><strong>Transmittance via <code>cumprod</code>:</strong> I compute the survival probability <code>T</code> using a cumulative product over <code>(1 − α)</code>. This is numerically stable and matches the project’s reference implementation.</li>
          <li><strong>Weights and colors:</strong> Each sample’s contribution is <code>w_i = T_i α_i</code>, meaning “the ray survives up to this point and terminates here.” I then take a weighted sum of the RGB values to get one color per ray.</li>
          <li><strong>Spec compatibility:</strong> The dual API (returning weights only when <code>t_vals</code> is passed) allows this function to both satisfy the autograder test and be reused in my own training code without extra wrappers.</li>
        </ol>
      </div>

      <h3>B.5 Training & Results on the Lego Scene</h3>
      <p>
        I train NeRF on the Lego dataset using Adam (learning rate 5e-4) with 10k rays per iteration. I use 96 samples per ray
        for Lego (<code>n_samples=96</code> between <code>near=2.0</code> and <code>far=6.0</code>), which gave a small PSNR boost
        over 64 samples while staying within the time budget.
      </p>
      <p>
        The training loop randomly samples rays across all views using a <code>RaysData</code> helper, then:
      </p>
      <ol>
        <li>Samples points along each ray with <code>sample_along_rays</code>.</li>
        <li>Feeds the points and directions into the <code>NeRF</code> MLP.</li>
        <li>Uses <code>volume_render</code> to get a predicted RGB color per ray.</li>
        <li>Computes MSE loss against ground-truth pixel colors and updates the network with Adam.</li>
      </ol>
      <p>
        In addition to the per-batch PSNR, I periodically call a separate <code>evaluate_nerf_psnr</code> routine that renders a fixed
        subset of rays from the first training image. This gives a stable, comparable PSNR curve over training.
      </p>

      <div class="grid grid-3">
        <figure>
          <img src="images/part2_lego_nerf/train_lego_iter0000.png" alt="Lego NeRF training iteration 0" />
          <figcaption>Lego NeRF at iteration 0 – essentially noise.</figcaption>
        </figure>
        <figure>
          <img src="images/part2_lego_nerf/train_lego_iter0200.png" alt="Lego NeRF training iteration 200" />
          <figcaption>Iteration 200 – rough geometry visible.</figcaption>
        </figure>
        <figure>
          <img src="images/part2_lego_nerf/train_lego_iter1000.png" alt="Lego NeRF training iteration 1000" />
          <figcaption>Iteration 1000 – refined textures and clear object boundaries.</figcaption>
        </figure>
      </div>

      <figure class="wide">
        <img src="images/part2_lego_nerf/psnr_curve_lego.png" alt="PSNR curve for Lego NeRF validation set" />
        <figcaption>PSNR vs training iteration for the Lego scene. The curve stabilizes once the network fits all views consistently.</figcaption>
      </figure>

      <figure class="wide">
        <img src="images/part2_lego_nerf/loss_curve_lego.png" alt="Loss curve for Lego NeRF validation set" />
        <figcaption>Loss vs training iteration for the Lego scene.</figcaption>
      </figure>

      <figure class="wide">
        <img src="images/part2_lego_nerf/viser_rays_and_samples_1.png" alt="Visualization of cameras, rays, and samples in Viser" />
        <figcaption>Viser visualization showing camera frustums, sampled rays, and 3D points along them.</figcaption>
      </figure>
      <figure class="wide">
        <img src="images/part2_lego_nerf/viser_rays_and_samples_2.png" alt="Visualization of cameras, rays, and samples in Viser" />
        <figcaption>Viser visualization showing camera frustums, sampled rays, and 3D points along them.</figcaption>
      </figure>

      <h3>B.6 Spherical Novel-View Rendering</h3>
      <p>
        To demonstrate true 3D understanding, I render novel views of the Lego scene from a spherical trajectory around the object.
        The NeRF is never explicitly told about these views; it synthesizes them from the learned radiance field.
      </p>

      <div class="grid grid-3">
        <figure>
          <img src="images/part2_lego_nerf/lego_spherical_frame00.png" alt="Lego spherical rendering frame 0" />
          <figcaption>Frame 0 – initial viewpoint from the training set’s general region.</figcaption>
        </figure>
        <figure>
          <img src="images/part2_lego_nerf/lego_spherical_frame20.png" alt="Lego spherical rendering frame 7" />
          <figcaption>Frame 7 – midway through the orbit, revealing previously occluded parts.</figcaption>
        </figure>
        <figure>
          <img src="images/part2_lego_nerf/lego_spherical_frame40.png" alt="Lego spherical rendering frame 14" />
          <figcaption>Frame 14 – later in the orbit, showing consistent geometry and appearance.</figcaption>
        </figure>
      </div>

      <figure class="wide">
        <img src="images/part2_lego_nerf/lego_spherical.gif" alt="Lego spherical orbit GIF" />
        <figcaption>Full spherical orbit GIF for the Lego scene, illustrating smooth novel views around the entire object.</figcaption>
      </figure>

      <p class="callout">
        <span class="label">Takeaway</span><br />
        The Lego experiment demonstrates that NeRF can recover a coherent 3D representation purely from multiple images and camera
        poses, without any explicit 3D supervision. Everything emerges from minimizing reconstruction error across views.
      </p>
    </section>

    <!-- Part C: My Own Object NeRF -->
    <section id="partC">
      <h2>Part C – NeRF on My Own Object (C.1–C.4)</h2>
      <p>
        Finally, I apply the entire NeRF pipeline to the dataset I captured in Part 0, training a NeRF that can synthesize novel
        views of my own object.
      </p>

      <h3>C.1 Dataset & Preprocessing</h3>
      <p>
        I use the undistorted images and <code>c2w</code> matrices produced earlier and package them into
        <code>images_train</code>, <code>images_val</code>, <code>c2ws_train</code>, and <code>c2ws_val</code>. I slightly adjust
        near/far bounds and number of samples to match the physical size of my scene (e.g., <code>near ≈ 0.02</code>,
        <code>far ≈ 0.5</code>, 64 samples per ray).
      </p>

      <h3>C.2 Training Behavior</h3>
      <p>
        Training behavior is similar to Lego but a bit more sensitive to hyperparameters, since my capture is less “perfect” than
        the synthetic dataset. The loss curve below shows the training loss decreasing over time.
      </p>

      <figure class="wide">
        <img src="images/part2_6_your_object/object_loss_curve.png" alt="Training loss curve for my object NeRF" />
        <figcaption>Training loss over iterations for my own object NeRF.</figcaption>
      </figure>

      <div class="grid grid-4">
        <figure>
          <img src="images/part2_6_your_object/object_intermediate_iter050.png" alt="NeRF of my object, iteration 50" />
          <figcaption>Iteration 50 – noisy but coarse silhouette is visible.</figcaption>
        </figure>
        <figure>
          <img src="images/part2_6_your_object/object_intermediate_iter200.png" alt="NeRF of my object, iteration 200" />
          <figcaption>Iteration 200 – main geometry emerges.</figcaption>
        </figure>
        <figure>
          <img src="images/part2_6_your_object/object_intermediate_iter500.png" alt="NeRF of my object, iteration 500" />
          <figcaption>Iteration 500 – textures sharpening.</figcaption>
        </figure>
        <figure>
          <img src="images/part2_6_your_object/object_intermediate_iter1000.png" alt="NeRF of my object, iteration 1000" />
          <figcaption>Iteration 1000 – final reconstruction.</figcaption>
        </figure>
      </div>

      <h3>C.3 Novel View Animation</h3>
      <p>
        I synthesize a small orbiting camera path around the object by creating new <code>c2w</code> matrices that place the camera
        on a circle, always looking at the origin. For each frame, I render an image using the trained NeRF and combine them into a GIF.
      </p>

      <div class="grid grid-3">
        <figure>
          <img src="images/part2_6_your_object/object_spherical_frame01.png" alt="My object spherical rendering frame 1" />
          <figcaption>Novel view frame 1.</figcaption>
        </figure>
        <figure>
          <img src="images/part2_6_your_object/object_spherical_frame02.png" alt="My object spherical rendering frame 2" />
          <figcaption>Novel view frame 2.</figcaption>
        </figure>
        <figure>
          <img src="images/part2_6_your_object/object_spherical_frame03.png" alt="My object spherical rendering frame 3" />
          <figcaption>Novel view frame 3.</figcaption>
        </figure>
      </div>

      <figure class="wide">
        <img src="images/part2_6_your_object/object_spherical.gif" alt="My object orbit GIF" />
        <figcaption>Full orbit GIF for my object, showing consistent geometry across viewpoints.</figcaption>
      </figure>

      <h3>C.4 Reflection</h3>
      <p>
        Compared to the Lego scene, my own data is noisier, less uniformly lit, and has fewer views. Nonetheless, NeRF is able to
        reconstruct a coherent 3D model that produces convincing novel views. This highlights both the power and fragility of
        the method: it can interpolate impressively, but is sensitive to calibration quality, coverage, and exposure consistency.
      </p>
    </section>

    <!-- Implementation Notes -->
    <section id="impl-notes" class="compact">
      <h2>Implementation Notes & Pitfalls</h2>
      <p>
        This section summarizes key parameter choices and practical issues I encountered while implementing NeRF from scratch.
      </p>
      <ul>
        <li><strong>Batch size:</strong> I used around 10k rays per iteration for Lego. Larger batches stabilize training but increase GPU memory usage.</li>
        <li><strong>Near/Far bounds:</strong> Getting these wrong either wastes samples (too wide) or chops off geometry (too tight).
          For my object I tuned <code>near</code> and <code>far</code> by visualizing depth and trying a few ranges.</li>
        <li><strong>Positional encoding frequencies:</strong> Very high frequencies can cause ringing artifacts if the network or data
          can’t support them. I found <code>pos_freqs≈10</code>, <code>dir_freqs≈4</code> to be a good balance.</li>
        <li><strong>Learning rate:</strong> For Lego I used <code>5e-4</code> with Adam. Higher learning rates converged faster but risked
          oscillations in PSNR.</li>
        <li><strong>Data quality:</strong> Slight miscalibration (especially in intrinsics) can produce subtle ghosting in the final NeRF.
          Carefully checking the Viser camera cloud was essential before committing to long training runs.</li>
        <li><strong>Debugging strategy:</strong> I verified each stage (ray directions, sampling locations, volume rendering) with small,
          synthetic tests before combining them. This helped isolate bugs that would otherwise manifest as “blurry renderings.”</li>
      </ul>

      <p class="callout">
        <span class="label">Lessons Learned</span><br />
        NeRF looks intimidating because of the integral in the volume rendering equation, but in code it’s mostly linear algebra and
        exponentials. The hard part isn’t the math—it’s keeping every coordinate system, normalization, and range consistent across
        the entire pipeline.
      </p>
    </section>

    <!-- References -->
    <section id="references" class="compact">
      <h2>References</h2>
      <ul>
        <li>
          Mildenhall et al., <em>NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis</em>, ECCV 2020.
        </li>
        <li>
          CS180: Intro to Computer Vision and Computational Photography – Project 4 spec and starter code.
        </li>
        <li>
          OpenCV documentation: camera calibration, ArUco detection, and PnP pose estimation.
        </li>
        <li>
          PyTorch documentation for building and training neural networks on GPU.
        </li>
      </ul>
    </section>
  </main>

  <footer>
    <p>© 2025 · CS180 Project 4 – Neural Radiance Fields.</p>
    <p>
      Some explanatory text and structure were drafted with the assistance of an AI model (ChatGPT, OpenAI) and then reviewed
      and edited by me. All implementation decisions, experiments, and final interpretations are my own.
    </p>
  </footer>
</body>
</html>
